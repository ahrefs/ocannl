Let's write a user-centered introduction to how shapes work in OCANNL. Let's put the slides in docs/slides-shapes_and_einsum.md , and write  them using slipshow navigation metadata as described in docs/CLAUDE.md . The slides should take a user from a beginner to advanced in making full use of shape inference and generalized einsum notation when  building neural network models. They should end up aware of how  projections work, how to lean on shape inference or row variables /  ellipsis notation to not commit to dimension sizes or for example the  number of batch axes unnecessarily. They should learn when to use the  dedicated einsum operators `++` and `+*` (these operators are  translated by syntax extensions to `einsum1` and `einsum`). They  should be able to use what they learned to construct a max pooling  layer operation, and any other challenges they encounter in NN  modeling.  Consider these sources of information: files docs/syntax_extensions.md , docs/shape_inference.md ,  docs/shape.mli , selected parts of lib/operation.ml , selected parts of docs/slides-basics_backprop_training_codegen.md .  Let me also provide some points that might not be stated sufficiently  explicitly in other documentation. (1) The split of axes into kinds  does not enforce semantics, because the generalized einsum notation  can make aribtrary use of the axes. However, it offers expressivity  gains: (a) outside of einsum spec, there is a shape logic  specification with syntax `~logic:"@"`, where all input axes of the  first tensor are reduced with all output axes of the second tensor,  generalizing matrix multiplication to tensor multiplication -- with  einsum spec, any two kinds of axes can be picked to reduce together,  but it would not be possible without having distinct kinds; (b) having multiple kinds, thus opportunity for multiple row variables per  tensor, allows more patterns of reorganizing and reducing axes, while  being agnostic to the total number of axes -- for example, one could  build code for a multihead-attention tranformer, that is agnostic  whether one uses one batch axis or two batch+microbatch axes, and  simultaneously is agnostic whether one uses one single-axis regular 1D attention or two-axes 2D axial attention, while handling the  head-number axis as needed. (2) It's important to stress the syntactic difference with NumPy: since we use `->` to separate input and output axes, it cannot mean separating the argument tensor(s) from the  result tensor -- thus `=>` is used to the left of the result tensor. (3) Remember to use kind separators where you intend to use the distinct axis kinds,  e.g use `|` after batch axes . (4) To trigger multichar mode there must be a comma in the spec, it can be a trailing comma e.g. "input->output, => output->input" . (5) A reminder that, as defined in lib/operation.ml , `*` stands for tensor multiplication and `*.` stands for pointwise  multiplication when working with tensor expressions (rather than  low-level assignments in the `%cd` syntax). (7) The user can define operations analogous to the  `einsum1` and `einsum` operation in lib/operation.ml , for example with the max operator as theaccumulation operator -- this is not so scary, operations can be easily added by users even if not inside lib/operation.ml .