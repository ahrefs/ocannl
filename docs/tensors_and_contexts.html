<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>tensors_and_contexts</title>
  <style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}

html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
  <style type="text/css">body {
max-width: 1000px;
margin: 0 auto;
padding: 20px;
}</style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#tensors-and-contexts" id="toc-tensors-and-contexts">Tensors and Contexts</a>
<ul>
<li><a href="#table-of-contents" id="toc-table-of-contents">Table of
Contents</a></li>
<li><a href="#core-concepts" id="toc-core-concepts">Core
Concepts</a></li>
<li><a href="#tensors" id="toc-tensors">Tensors</a>
<ul>
<li><a href="#tensor-structure" id="toc-tensor-structure">Tensor
Structure</a></li>
<li><a href="#roots-embedded-nodes-and-params" id="toc-roots-embedded-nodes-and-params">Roots, Embedded Nodes, and
Params</a></li>
<li><a href="#creating-tensors" id="toc-creating-tensors">Creating
Tensors</a></li>
<li><a href="#operation-functions-and-shape-parameters" id="toc-operation-functions-and-shape-parameters">Operation Functions
and Shape Parameters</a></li>
<li><a href="#gradient-specifications" id="toc-gradient-specifications">Gradient Specifications</a></li>
<li><a href="#tensor-precision" id="toc-tensor-precision">Tensor
Precision</a></li>
</ul></li>
<li><a href="#contexts" id="toc-contexts">Contexts</a>
<ul>
<li><a href="#context-creation" id="toc-context-creation">Context
Creation</a></li>
<li><a href="#compilation-and-execution" id="toc-compilation-and-execution">Compilation and Execution</a></li>
<li><a href="#node-initialization-tracking" id="toc-node-initialization-tracking">Node Initialization
Tracking</a></li>
</ul></li>
<li><a href="#the-train-module" id="toc-the-train-module">The Train
Module</a>
<ul>
<li><a href="#parameter-initialization" id="toc-parameter-initialization">Parameter Initialization</a></li>
<li><a href="#forward-and-backward-passes" id="toc-forward-and-backward-passes">Forward and Backward
Passes</a></li>
<li><a href="#training-loops" id="toc-training-loops">Training
Loops</a></li>
</ul></li>
<li><a href="#randomness-and-parameter-initialization" id="toc-randomness-and-parameter-initialization">Randomness and
Parameter Initialization</a>
<ul>
<li><a href="#counter-based-prng" id="toc-counter-based-prng">Counter-Based PRNG</a></li>
<li><a href="#randomness-operations" id="toc-randomness-operations">Randomness Operations</a></li>
<li><a href="#kaiming-and-xavier-scaling-operations" id="toc-kaiming-and-xavier-scaling-operations">Kaiming and Xavier
Scaling Operations</a></li>
<li><a href="#parameter-initialization-1" id="toc-parameter-initialization-1">Parameter Initialization</a></li>
</ul></li>
<li><a href="#memory-management" id="toc-memory-management">Memory
Management</a></li>
<li><a href="#further-reading" id="toc-further-reading">Further
Reading</a></li>
</ul></li>
</ul>
</nav>
<h1 id="tensors-and-contexts">Tensors and Contexts</h1>
<p>This document describes how to work with tensors and execution
contexts in OCANNL. It covers the core <code>Tensor</code> module, the
<code>Context</code> API for backend management, and the
<code>Train</code> module for training workflows.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#core-concepts">Core Concepts</a></li>
<li><a href="#tensors">Tensors</a>
<ul>
<li><a href="#tensor-structure">Tensor Structure</a></li>
<li><a href="#roots-embedded-nodes-and-params">Roots, Embedded Nodes,
and Params</a></li>
<li><a href="#creating-tensors">Creating Tensors</a></li>
<li><a href="#operation-functions-and-shape-parameters">Operation
Functions and Shape Parameters</a></li>
<li><a href="#gradient-specifications">Gradient Specifications</a></li>
<li><a href="#tensor-precision">Tensor Precision</a></li>
</ul></li>
<li><a href="#contexts">Contexts</a>
<ul>
<li><a href="#context-creation">Context Creation</a></li>
<li><a href="#compilation-and-execution">Compilation and
Execution</a></li>
<li><a href="#node-initialization-tracking">Node Initialization
Tracking</a></li>
</ul></li>
<li><a href="#the-train-module">The Train Module</a>
<ul>
<li><a href="#parameter-initialization">Parameter
Initialization</a></li>
<li><a href="#forward-and-backward-passes">Forward and Backward
Passes</a></li>
<li><a href="#training-loops">Training Loops</a></li>
</ul></li>
<li><a href="#randomness-and-parameter-initialization">Randomness and
Parameter Initialization</a>
<ul>
<li><a href="#counter-based-prng">Counter-Based PRNG</a></li>
<li><a href="#initialization-functions">Initialization
Functions</a></li>
<li><a href="#kaiming-and-xavier-initialization">Kaiming and Xavier
Initialization</a></li>
<li><a href="#configuring-default-initialization">Configuring Default
Initialization</a></li>
</ul></li>
<li><a href="#memory-management">Memory Management</a></li>
</ul>
<h2 id="core-concepts">Core Concepts</h2>
<p>OCANNL separates the <em>definition</em> of computations from their
<em>execution</em>:</p>
<ol type="1">
<li><strong>Tensors</strong> (<code>Tensor.t</code>) describe
computations and their structure, including forward code, gradients, and
backpropagation logic.</li>
<li><strong>Contexts</strong> (<code>Context.t</code>) manage execution
on a specific backend (CPU, CUDA, Metal), handling device memory and
compiled routines.</li>
<li><strong>Routines</strong> (<code>Context.routine</code>) are
compiled computations ready for efficient repeated execution.</li>
</ol>
<p>This separation enables: - Defining models once, running on multiple
backends - Compiling training loops once, executing many times
efficiently - Clear separation between model definition and execution
concerns</p>
<h2 id="tensors">Tensors</h2>
<h3 id="tensor-structure">Tensor Structure</h3>
<p>A tensor in OCANNL (<code>Tensor.t</code>) contains:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> t = {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  params : (t, comparator_witness) Base.<span class="dt">Set</span>.t;  <span class="co">(* Learnable parameters *)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  forward : comp;                                <span class="co">(* Forward computation *)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  diff : diff <span class="dt">option</span>;                            <span class="co">(* Gradient info if differentiable *)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  id : <span class="dt">int</span>;                                      <span class="co">(* Unique identifier *)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  value : tn;                                    <span class="co">(* The underlying tensor node *)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  shape : Shape.t;                               <span class="co">(* Shape with inference state *)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  children : subtensor <span class="dt">list</span>;                     <span class="co">(* Sub-tensors in the computation *)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">(* ... *)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Key fields: - <strong><code>value</code></strong>: The tensor node
(<code>Ir.Tnode.t</code>) holding the actual data -
<strong><code>diff</code></strong>: Contains gradient node and
backpropagation code (if differentiable) -
<strong><code>params</code></strong>: Set of tensors requiring separate
initialization (see <a href="#roots-embedded-nodes-and-params">Roots,
Embedded Nodes, and Params</a>) - <strong><code>forward</code></strong>:
Computation to produce this tensor’s value, including embedded subtensor
computations - <strong><code>shape</code></strong>: Shape information,
progressively refined during inference</p>
<h3 id="roots-embedded-nodes-and-params">Roots, Embedded Nodes, and
Params</h3>
<p>Understanding how OCANNL manages computation inclusion is essential
for advanced usage.</p>
<h4 id="forward-roots-and-embedding">Forward Roots and Embedding</h4>
<p>When you build a computation graph, OCANNL tracks <strong>forward
roots</strong> - tensors whose forward computation code hasn’t yet been
included in another tensor’s forward code. When a tensor T uses
subtensor S:</p>
<ol type="1">
<li>If S is a forward root, S’s forward code is
<strong>embedded</strong> into T’s forward code</li>
<li>S is then removed from the forward roots set</li>
<li>S’s <code>value</code> node is added to T’s
<code>forward.embedded_nodes</code></li>
</ol>
<p>This ensures each tensor’s initialization code appears exactly once -
in the first tensor that uses it. The <code>embedded_nodes</code> set
tracks which tensor nodes’ computations are included in a given forward
computation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* When creating: let%op result = a + b *)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">(* If &#39;a&#39; is a forward root:</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">   - a.forward code is included in result.forward</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">   - a.value is added to result.forward.embedded_nodes</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">   - &#39;a&#39; is removed from forward_roots *)</span></span></code></pre></div>
<h4 id="the-params-field">The <code>params</code> Field</h4>
<p>The <code>params</code> field of a tensor is slightly misnamed from a
user perspective. It contains tensors that: 1. Need initialization
(their forward code computes initial values) 2. Are NOT embedded in the
parent tensor’s forward code</p>
<p>This typically corresponds to “parameters” in the ML sense (learnable
weights), but more precisely it means “tensors requiring separate
initialization”. When you call <code>Train.init_params</code>, it: 1.
Collects all tensors in <code>t.params</code> recursively 2. Gathers
their <code>forward.embedded_nodes</code> 3. Compiles and runs their
initialization code</p>
<p>A tensor becomes a “param” when created via <code>Tensor.param</code>
or inline <code>{ name }</code> syntax - its forward code is kept
separate rather than embedded into the computation that uses it.</p>
<h4 id="why-this-matters">Why This Matters</h4>
<p>This design enables efficient execution: - Initialization code runs
once, not every forward pass - Forward pass code only includes the
actual computation - Context tracks which nodes are initialized to
remind the programmer of potential bugs - Tensors can be computed in one
routine and used in another routine</p>
<p><strong>Potential pitfall</strong>: If a tensor T1 embeds another
tensor S (capturing S’s forward code), but T1 is never used in the final
computation, S’s initialization may be “lost” - it won’t appear in
<code>init_params</code> because T1 isn’t in anyone’s
<code>params</code>. OCANNL handles the common case of
<code>random_seed</code> specially, but custom initialization patterns
may need care.</p>
<h4 id="backprop-roots">Backprop Roots</h4>
<p>Similarly, <strong>backprop roots</strong> track tensors whose
backpropagation code hasn’t been consumed. When you call
<code>Train.grad_update</code> or similar, the backprop code flows from
loss backward through the graph, with each tensor’s backprop code
included exactly once.</p>
<h3 id="creating-tensors">Creating Tensors</h3>
<p>OCANNL provides multiple ways to create tensors:</p>
<p><strong>Using DSL modules</strong>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">open</span> Ocannl.Operation.DSL_modules</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">(* Differentiable tensor with gradient tracking when needed *)</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> x = TDSL.term ~output_dims:[<span class="dv">10</span>] ()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">(* Non-differentiable tensor (no gradient) *)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> const = NTDSL.number <span class="fl">3.14</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">(* Parameter with gradient (for model weights) *)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> w = TDSL.param <span class="st">&quot;weights&quot;</span> ~output_dims:[hidden_dim] ()</span></code></pre></div>
<p><strong>Using syntax extensions</strong> (see <a href="syntax_extensions.md">syntax_extensions.md</a>):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">open</span> Ocannl.Operation.DSL_modules</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">(* %op creates differentiable tensors *)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op layer x = { w } * x + { b = <span class="dv">0</span>.; o = [hidden_dim] }</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">(* %cd creates assignment code with non-differentiable intermediate tensors *)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%cd update = p =- learning_rate *. p.grad</span></code></pre></div>
<h3 id="operation-functions-and-shape-parameters">Operation Functions
and Shape Parameters</h3>
<p>Most tensor-creating operations in OCANNL return an <strong>operation
function</strong> (<code>op_fun</code>) rather than a tensor directly.
This allows you to specify shape constraints and other options at the
call site:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* param_op_fun: the innermost layer of optional parameters *)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> param_op_fun =</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  ?input_dims:<span class="dt">int</span> <span class="dt">list</span> -&gt;</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  ?output_dims:<span class="dt">int</span> <span class="dt">list</span> -&gt;</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  ?input_axes:(<span class="dt">string</span> * <span class="dt">int</span>) <span class="dt">list</span> -&gt;</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  ?output_axes:(<span class="dt">string</span> * <span class="dt">int</span>) <span class="dt">list</span> -&gt;</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  ?deduced:Shape.deduce_within_shape -&gt;</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">unit</span> -&gt;</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  t</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">(* op_fun: extends param_op_fun with label and precision options *)</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> op_fun =</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  ?label:<span class="dt">string</span> <span class="dt">list</span> -&gt;</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  ?top_down_prec:<span class="dt">bool</span> -&gt;</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  ?batch_dims:<span class="dt">int</span> <span class="dt">list</span> -&gt;</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  ?batch_axes:(<span class="dt">string</span> * <span class="dt">int</span>) <span class="dt">list</span> -&gt;</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  param_op_fun</span></code></pre></div>
<p><strong>Key parameters:</strong></p>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>~input_dims:[n; m]</code></td>
<td>Set input axis dimensions (rightmost in memory layout)</td>
</tr>
<tr>
<td><code>~output_dims:[p; q]</code></td>
<td>Set output axis dimensions</td>
</tr>
<tr>
<td><code>~batch_dims:[b]</code></td>
<td>Set batch axis dimensions (leftmost in memory layout)</td>
</tr>
<tr>
<td><code>~input_axes:[(&quot;hidden&quot;, 64)]</code></td>
<td>Input axes with labeled dimensions</td>
</tr>
<tr>
<td><code>~output_axes:[(&quot;classes&quot;, 10)]</code></td>
<td>Output axes with labeled dimensions</td>
</tr>
<tr>
<td><code>~label:[&quot;layer1&quot;]</code></td>
<td>Labels for debugging/printing</td>
</tr>
<tr>
<td><code>~top_down_prec:true</code></td>
<td>Force precision from parent operation</td>
</tr>
<tr>
<td><code>~deduced:Input_equals_output</code></td>
<td>Shape constraint (input = output dims)</td>
</tr>
<tr>
<td><code>()</code></td>
<td><strong>Required</strong>: finalizes the tensor creation</td>
</tr>
</tbody>
</table>
<p><strong>Usage examples:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Operations like pointmul return op_fun, allowing shape specification *)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> scaled = pointmul x scale ~output_dims:[hidden_dim] ()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* The kaiming/xavier functions return op_fun for deferred shape binding *)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> weights = kaiming uniform1 ~input_dims:[<span class="dv">784</span>] ~output_dims:[<span class="dv">256</span>] ()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Term creates a terminal tensor with shape *)</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> <span class="dt">input</span> = TDSL.term ~batch_dims:[batch_size] ~input_dims:[<span class="dv">784</span>] ()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">(* Params use the same interface *)</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> w = TDSL.param <span class="st">&quot;weights&quot;</span> ~input_dims:[in_dim] ~output_dims:[out_dim] ()</span></code></pre></div>
<p><strong>Why this design?</strong></p>
<ol type="1">
<li><strong>Deferred shape binding</strong>: Shape can be specified at
the use site rather than definition site</li>
<li><strong>Shape inference integration</strong>: Unspecified dimensions
are inferred from context</li>
<li><strong>Flexible composition</strong>: Functions like
<code>kaiming</code> can wrap other functions while preserving the
interface</li>
</ol>
<p>For example, <code>kaiming_at init_f counter</code> returns an
<code>op_fun</code>, so you can write:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Shape specified at call site, not in kaiming_at definition *)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> w = kaiming_at uniform_at counter ~input_dims:[<span class="dv">100</span>] ~output_dims:[<span class="dv">40</span>] ()</span></code></pre></div>
<h3 id="gradient-specifications">Gradient Specifications</h3>
<p>The <code>grad_spec</code> parameter controls gradient behavior:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> grad_spec = Require_grad | Prohibit_grad | If_needed</span></code></pre></div>
<ul>
<li><strong><code>Require_grad</code></strong>: Always create gradient
nodes (for learnable parameters)</li>
<li><strong><code>Prohibit_grad</code></strong>: Never create gradients
(for constants, inputs)</li>
<li><strong><code>If_needed</code></strong>: Create gradients only if
required by the computation graph</li>
</ul>
<p>The DSL modules set appropriate defaults:</p>
<ul>
<li><code>TDSL</code> uses <code>If_needed</code> (automatic gradient
propagation)</li>
<li><code>NTDSL</code> uses <code>Prohibit_grad</code>
(non-differentiable)</li>
<li><code>PDSL</code> uses <code>Require_grad</code> (parameters)</li>
</ul>
<h3 id="tensor-precision">Tensor Precision</h3>
<p>OCANNL supports multiple numeric precisions:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Tensor.default_value_prec := Ir.Ops.single  <span class="co">(* float32, the default *)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>Tensor.default_grad_prec := Ir.Ops.single</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Available precisions *)</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>Ir.Ops.half    <span class="co">(* float16 *)</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>Ir.Ops.single  <span class="co">(* float32 *)</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>Ir.Ops.double  <span class="co">(* float64 *)</span></span></code></pre></div>
<p>Individual tensor nodes can have their precision updated:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Ir.Tnode.update_prec tensor.value Ir.Ops.double</span></code></pre></div>
<p>See <a href="docs/precision_inference.md">Precision Inference</a> for
details.</p>
<h2 id="contexts">Contexts</h2>
<p>The <code>Context</code> module provides a simplified interface for
backend management, introduced in v0.6.1.</p>
<h3 id="context-creation">Context Creation</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Automatic backend selection (respects OCANNL_BACKEND env var) *)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.auto ()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Explicit backend selection *)</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.cuda ~device_id:<span class="dv">0</span> ()  <span class="co">(* NVIDIA GPU *)</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.metal ~device_id:<span class="dv">0</span> () <span class="co">(* Apple Metal *)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.cpu ~threads:<span class="dv">4</span> ()     <span class="co">(* Multi-threaded CPU *)</span></span></code></pre></div>
<h3 id="compilation-and-execution">Compilation and Execution</h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Compile a computation *)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx, routine = Context.compile ctx computation bindings</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Execute the routine *)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.run ctx routine</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Access routine metadata *)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> bindings = Context.bindings routine</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.context routine</span></code></pre></div>
<h3 id="node-initialization-tracking">Node Initialization Tracking</h3>
<p>Contexts track which tensor nodes have been initialized:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Check if a node is initialized *)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> is_init = Context.is_initialized ctx tensor.value</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Initialize from host memory *)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.init_from_host_deprecated ctx tensor.value</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Copy between contexts *)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>Context.copy ~src:ctx1 ~dst:ctx2 tensor.value</span></code></pre></div>
<h2 id="the-train-module">The Train Module</h2>
<p>The <code>Train</code> module provides high-level utilities for
training workflows.</p>
<h3 id="parameter-initialization">Parameter Initialization</h3>
<div class="sourceCode" id="cb14"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Initialize all parameters of a tensor *)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Train.init_params ctx bindings loss</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* With options *)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Train.init_params</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  ~reinit_all:<span class="kw">true</span>      <span class="co">(* Reinitialize even if already initialized *)</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  ~hosted:<span class="kw">true</span>          <span class="co">(* Keep values accessible on host *)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  ctx bindings loss</span></code></pre></div>
<h3 id="forward-and-backward-passes">Forward and Backward Passes</h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Forward pass only *)</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Train.forward_once ctx tensor</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Forward + backward (gradient update) *)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Train.update_once ctx loss</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Build gradient update computation *)</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> update = Train.grad_update loss</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">(* Returns: forward + zero_grads + set grad to 1 + backprop *)</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">(* Build SGD parameter update *)</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> sgd = Train.sgd_update ~learning_rate loss</span></code></pre></div>
<h3 id="training-loops">Training Loops</h3>
<p>For efficient training, compile routines once and execute
repeatedly:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Context.auto () <span class="kw">in</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> step_n, bindings = IDX.get_static_symbol IDX.empty <span class="kw">in</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Define model and loss *)</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op loss = ... <span class="kw">in</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Build update computations *)</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op learning_rate = <span class="fl">0.01</span> <span class="kw">in</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> update = Train.grad_update loss <span class="kw">in</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> sgd = Train.sgd_update ~learning_rate loss <span class="kw">in</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">(* Initialize parameters *)</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> ctx = Train.init_params ctx bindings loss <span class="kw">in</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">(* Compile training routine *)</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> routine = Train.to_routine ctx bindings</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  (Asgns.sequence [update; sgd]) <span class="kw">in</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">(* Training loop *)</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> step_ref = IDX.find_exn (Context.bindings routine) step_n <span class="kw">in</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="kw">for</span> step = <span class="dv">1</span> <span class="kw">to</span> num_steps <span class="kw">do</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>  step_ref := step;</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>  Train.run ctx routine;</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">if</span> step <span class="kw">mod</span> <span class="dv">100</span> = <span class="dv">0</span> <span class="kw">then</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    printf <span class="st">&quot;Step %d, Loss: %.4f</span><span class="ch">\n</span><span class="st">&quot;</span> step loss.@[<span class="dv">0</span>]</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="kw">done</span></span></code></pre></div>
<p>See <a href="migration_guide.md">migration_guide.md</a> for more
training patterns.</p>
<h2 id="randomness-and-parameter-initialization">Randomness and
Parameter Initialization</h2>
<p>OCANNL uses a deterministic, counter-based approach to random number
generation, which differs significantly from traditional imperative
RNGs.</p>
<h3 id="counter-based-prng">Counter-Based PRNG</h3>
<p>OCANNL implements the Threefry algorithm, a counter-based PRNG
that:</p>
<ul>
<li>Produces deterministic sequences from a seed and counter</li>
<li>Enables reproducible initialization across runs</li>
<li>Supports parallel random generation without synchronization</li>
</ul>
<p>The random seed is managed globally:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Set the random seed *)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>Tensor.set_random_seed ~seed:<span class="dv">42</span> ()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Or configure via settings before tensor creation *)</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>Utils.settings.fixed_state_for_init &lt;- <span class="dt">Some</span> <span class="dv">42</span>;</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>Tensor.unsafe_reinitialize ()</span></code></pre></div>
<h3 id="randomness-operations">Randomness Operations</h3>
<p>OCANNL provides several random operations in the DSL modules:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 43%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr>
<th>Function</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>uniform ()</code></td>
<td>Uniform [0,1) using global seed</td>
<td>Efficient, requires shape divisibility</td>
</tr>
<tr>
<td><code>uniform1 ()</code></td>
<td>Uniform [0,1), pointwise</td>
<td>Works with any shape, less efficient</td>
</tr>
<tr>
<td><code>uniform_at counter</code></td>
<td>Uniform using explicit counter</td>
<td>For training-time randomness</td>
</tr>
<tr>
<td><code>uniform_at1 counter</code></td>
<td>Pointwise uniform with counter</td>
<td>For training-time, any shape</td>
</tr>
<tr>
<td><code>normal ()</code>, <code>normal1 ()</code></td>
<td>Standard normal N(0,1)</td>
<td>Uses Box-Muller transform</td>
</tr>
<tr>
<td><code>normal_at counter</code>, <code>normal_at1 counter</code></td>
<td>Normal with explicit counter</td>
<td>For training-time randomness</td>
</tr>
</tbody>
</table>
<p><strong>Important</strong>: The <code>counter</code> argument in
<code>_at</code> variants is for <strong>randomness
bifurcation</strong>, not shape determination. The counter should be
scalar / dimension-1 (e.g., a training step number). Different counter
values produce different random streams across the resulting tensor
cells.</p>
<p>The output shape is determined by: 1. <strong>Shape
inference</strong> from how the result is used (e.g., pointwise ops with
shaped tensors) 2. <strong>Explicit dimensions</strong> via
<code>TDSL.uniform_at ~output_dims:[...] counter ()</code></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Dropout with training-step-dependent randomness *)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op dropout ~rate () ~train_step x =</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">match</span> train_step <span class="kw">with</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  | <span class="dt">Some</span> train_step <span class="kw">when</span> Float.(rate &gt; <span class="fl">0.0</span>) -&gt;</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>      <span class="co">(* !@train_step embeds the step counter as a scalar tensor *)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>      <span class="co">(* Shape is inferred from pointwise comparison with x *)</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>      x *. (!.rate &lt; uniform_at !@train_step) /. (<span class="fl">1.0</span> - !.rate)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  | _ -&gt; x</span></code></pre></div>
<p><strong>Note on shape constraints</strong>: The <code>uniform</code>
function (without <code>1</code>) requires the total number of elements
to be appropriately divisible, e.g. by 4 for single precision (due to
<code>uint4x32</code> efficiency). Use <code>uniform1</code> for
arbitrary shapes at the cost of some efficiency.</p>
<h3 id="kaiming-and-xavier-scaling-operations">Kaiming and Xavier
Scaling Operations</h3>
<p>For weight matrices, OCANNL provides scaled initialization functions
that use shape inference to determine the scaling factor:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Kaiming (He) initialization: sqrt(scale_sq / fan_in) scaling, default scale_sq=6 *)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> kaiming ?scale_sq init_f () = ...</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Xavier (Glorot) initialization: sqrt(scale_sq / (fan_in + fan_out)) scaling *)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> xavier ?scale_sq init_f () = ...</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Counter-based variants for deferred initialization *)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> kaiming_at ?scale_sq init_f counter = ...</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> xavier_at ?scale_sq init_f counter = ...</span></code></pre></div>
<p>These functions use einsum dimension capture to extract fan_in and
fan_out:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* kaiming_impl captures input dimension *)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op _ = w_raw ++ <span class="st">&quot;...|..i.. -&gt; ... =&gt; 0&quot;</span> [ <span class="st">&quot;i&quot;</span> ] <span class="kw">in</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>... <span class="dt">sqrt</span> (!.scale_sq /. dim i)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">(* xavier_impl captures both input and output dimensions *)</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op _ = w_raw ++ <span class="st">&quot;...|..i.. -&gt; ..o.. =&gt; 0&quot;</span> [ <span class="st">&quot;i&quot;</span>; <span class="st">&quot;o&quot;</span> ] <span class="kw">in</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>... <span class="dt">sqrt</span> (!.scale_sq /. (dim i + dim o))</span></code></pre></div>
<h3 id="parameter-initialization-1">Parameter Initialization</h3>
<p>Usage example:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Set kaiming initialization as default: PDSL outside, TDSL inside *)</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>TDSL.default_param_init := PDSL.kaiming TDSL.O.uniform1;</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Or use directly in parameter definition *)</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op layer x = { w = kaiming uniform1 () } * x + { b = <span class="dv">0</span>. }</span></code></pre></div>
<p>When setting <code>default_param_init</code>, we call
<code>PDSL.kaiming</code> so that the result is differentiable, but
<code>TDSL.O.uniform1</code> or <code>NTDSL.O.uniform1</code> so the
intermediate values are not differentiable.</p>
<h2 id="memory-management">Memory Management</h2>
<p>OCANNL manages tensor memory through memory modes:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Make a tensor&#39;s value accessible on host *)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>Train.set_hosted tensor.value</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Set as materialized (on-device only) *)</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>Train.set_materialized tensor.value</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">(* Set as virtual (inlined during compilation) *)</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>Train.set_virtual tensor.value</span></code></pre></div>
<p>The <code>Train</code> module functions often set appropriate memory
modes automatically: - <code>forward</code> and
<code>forward_once</code> set the result as hosted -
<code>init_params</code> sets parameter values as hosted by default</p>
<p>For advanced memory control, see <a href="lowering_and_inlining.md">lowering_and_inlining.md</a>.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="syntax_extensions.md">Syntax Extensions</a> -
<code>%op</code> and <code>%cd</code> syntax details</li>
<li><a href="shape_inference.md">Shape Inference</a> - How shapes are
inferred</li>
<li><a href="migration_guide.md">Migration Guide</a> - Coming from
PyTorch/TensorFlow</li>
<li><a href="anatomy_of_a_backend.md">Anatomy of a Backend</a> - Backend
implementation details</li>
</ul>
</body>
</html>
