<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>migration_guide</title>
  <style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}

html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
  <style type="text/css">body {
max-width: 1000px;
margin: 0 auto;
padding: 20px;
}</style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#migration-guide-pytorchtensorflow-to-ocannl" id="toc-migration-guide-pytorchtensorflow-to-ocannl">Migration Guide:
PyTorch/TensorFlow to OCANNL</a>
<ul>
<li><a href="#key-conceptual-differences" id="toc-key-conceptual-differences">Key Conceptual Differences</a>
<ul>
<li><a href="#shape-inference-vs-explicit-shapes" id="toc-shape-inference-vs-explicit-shapes">Shape Inference vs Explicit
Shapes</a></li>
<li><a href="#two-phase-inference-system" id="toc-two-phase-inference-system">Two-Phase Inference System</a></li>
</ul></li>
<li><a href="#common-operations-mapping" id="toc-common-operations-mapping">Common Operations Mapping</a></li>
<li><a href="#tensor-creation-patterns" id="toc-tensor-creation-patterns">Tensor Creation Patterns</a>
<ul>
<li><a href="#parameters-learnable-tensors" id="toc-parameters-learnable-tensors">Parameters (Learnable
Tensors)</a></li>
<li><a href="#non-learnable-constants" id="toc-non-learnable-constants">Non-learnable Constants</a></li>
</ul></li>
<li><a href="#network-architecture-patterns" id="toc-network-architecture-patterns">Network Architecture Patterns</a>
<ul>
<li><a href="#sequential-models" id="toc-sequential-models">Sequential
Models</a></li>
<li><a href="#residual-connections" id="toc-residual-connections">Residual Connections</a></li>
</ul></li>
<li><a href="#einsum-notation" id="toc-einsum-notation">Einsum
Notation</a>
<ul>
<li><a href="#syntax-modes" id="toc-syntax-modes">Syntax Modes</a></li>
<li><a href="#row-variables" id="toc-row-variables">Row
Variables</a></li>
</ul></li>
<li><a href="#common-gotchas-and-solutions" id="toc-common-gotchas-and-solutions">Common Gotchas and Solutions</a>
<ul>
<li><a href="#variable-capture-with-einsum" id="toc-variable-capture-with-einsum">Variable Capture with
Einsum</a></li>
<li><a href="#creating-non-learnable-constants" id="toc-creating-non-learnable-constants">Creating Non-learnable
Constants</a></li>
<li><a href="#parameter-scoping" id="toc-parameter-scoping">Parameter
Scoping</a></li>
<li><a href="#flattening-for-linear-layers" id="toc-flattening-for-linear-layers">Flattening for Linear
Layers</a></li>
</ul></li>
<li><a href="#training-loop-patterns" id="toc-training-loop-patterns">Training Loop Patterns</a>
<ul>
<li><a href="#basic-training-step" id="toc-basic-training-step">Basic
Training Step</a></li>
</ul></li>
<li><a href="#debugging-tips" id="toc-debugging-tips">Debugging Tips</a>
<ul>
<li><a href="#shape-errors" id="toc-shape-errors">Shape Errors</a></li>
<li><a href="#type-errors-with-inline-definitions" id="toc-type-errors-with-inline-definitions">Type Errors with Inline
Definitions</a></li>
<li><a href="#performance" id="toc-performance">Performance</a></li>
</ul></li>
<li><a href="#random-number-generation-details" id="toc-random-number-generation-details">Random Number Generation
Details</a>
<ul>
<li><a href="#initialization-functions" id="toc-initialization-functions">Initialization Functions</a></li>
</ul></li>
<li><a href="#further-resources" id="toc-further-resources">Further
Resources</a></li>
</ul></li>
</ul>
</nav>
<h1 id="migration-guide-pytorchtensorflow-to-ocannl">Migration Guide:
PyTorch/TensorFlow to OCANNL</h1>
<p>This guide helps deep learning practitioners familiar with PyTorch or
TensorFlow understand OCANNL’s approach and idioms.</p>
<h2 id="key-conceptual-differences">Key Conceptual Differences</h2>
<h3 id="shape-inference-vs-explicit-shapes">Shape Inference vs Explicit
Shapes</h3>
<ul>
<li><p><strong>PyTorch/TF</strong>: Shapes are usually explicit (e.g.,
<code>Conv2d(in_channels=3, out_channels=64)</code>)</p></li>
<li><p><strong>OCANNL</strong>: Shapes are inferred where possible,
using row variables for flexibility</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Channels as row variables allow multi-channel architectures *)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>conv2d ~label () x  <span class="co">(* ..ic.. and ..oc.. are inferred *)</span></span></code></pre></div></li>
</ul>
<h3 id="two-phase-inference-system">Two-Phase Inference System</h3>
<p>OCANNL separates shape inference from projection inference: -
<strong>Shape inference</strong>: Global, propagates constraints across
operations - <strong>Projection inference</strong>: Local
per-assignment, derives loop structures from tensor shapes</p>
<p>This is why pooling needs a dummy constant kernel - to carry shape
info between phases.</p>
<h2 id="common-operations-mapping">Common Operations Mapping</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 40%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>PyTorch/TensorFlow</th>
<th>OCANNL</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x.view(-1, d)</code> or <code>x.reshape(-1, d)</code></td>
<td>Not supported yet</td>
<td>Use shape inference and let tensors have the shape they want</td>
</tr>
<tr>
<td><code>x.flatten()</code></td>
<td>Not supported yet</td>
<td>Future syntax might be: <code>&quot;x,y =&gt; x&amp;y&quot;</code></td>
</tr>
<tr>
<td><code>nn.Conv2d(in_c, out_c, kernel_size=k)</code></td>
<td><code>conv2d ~kernel_size:k () x</code></td>
<td>Channels inferred or use row vars</td>
</tr>
<tr>
<td><code>F.max_pool2d(x, kernel_size=k)</code></td>
<td><code>max_pool2d ~window_size:k () x</code></td>
<td>Uses <code>(0.5 + 0.5)</code> trick internally</td>
</tr>
<tr>
<td><code>F.avg_pool2d(x, kernel_size=k)</code></td>
<td><code>avg_pool2d ~window_size:k () x</code></td>
<td>Normalized by window size</td>
</tr>
<tr>
<td><code>nn.BatchNorm2d(channels)</code></td>
<td><code>batch_norm2d () ~train_step x</code></td>
<td>Channels inferred</td>
</tr>
<tr>
<td><code>F.dropout(x, p=0.5)</code></td>
<td><code>dropout ~rate:0.5 () ~train_step x</code></td>
<td>Needs train_step for PRNG</td>
</tr>
<tr>
<td><code>F.relu(x)</code></td>
<td><code>relu x</code></td>
<td>Direct function application</td>
</tr>
<tr>
<td><code>F.softmax(x, dim=-1)</code></td>
<td><code>softmax ~spec:&quot;... | ... -&gt; ... d&quot; () x</code></td>
<td>Specify axes explicitly</td>
</tr>
<tr>
<td><code>torch.matmul(a, b)</code></td>
<td><code>a * b</code> or <code>a +* &quot;...; ... =&gt; ...&quot; b</code></td>
<td>Einsum for complex cases</td>
</tr>
<tr>
<td><code>x.mean(dim=[1,2])</code></td>
<td><code>x ++ &quot;... | h, w, c =&gt; ... | 0, 0, c&quot; [&quot;h&quot;; &quot;w&quot;] /. (dim h *. dim w)</code></td>
<td>Sum then divide</td>
</tr>
<tr>
<td><code>x.sum(dim=-1)</code></td>
<td><code>x ++ &quot;... | ... d =&gt; ... | 0&quot;</code></td>
<td>Reduce by summing</td>
</tr>
</tbody>
</table>
<h2 id="tensor-creation-patterns">Tensor Creation Patterns</h2>
<h3 id="parameters-learnable-tensors">Parameters (Learnable
Tensors)</h3>
<table>
<thead>
<tr>
<th>PyTorch</th>
<th>OCANNL</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nn.Parameter(torch.rand(d))</code></td>
<td><code>{ w }</code> or <code>{ w = uniform () }</code></td>
</tr>
<tr>
<td><code>nn.Parameter(torch.randn(d))</code></td>
<td><code>{ w = normal () }</code></td>
</tr>
<tr>
<td><code>nn.Parameter(torch.zeros(d))</code></td>
<td><code>{ w = 0. }</code></td>
</tr>
<tr>
<td><code>nn.Parameter(torch.ones(d))</code></td>
<td><code>{ w = 1. }</code></td>
</tr>
<tr>
<td>With explicit dims</td>
<td><code>{ w; o = [out_dim]; i = [in_dim] }</code></td>
</tr>
</tbody>
</table>
<h3 id="non-learnable-constants">Non-learnable Constants</h3>
<table>
<colgroup>
<col style="width: 34%" />
<col style="width: 34%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th>PyTorch</th>
<th>OCANNL</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>torch.ones_like(x)</code></td>
<td><code>0.5 + 0.5</code></td>
<td>Shape-inferred constant 1</td>
</tr>
<tr>
<td><code>torch.tensor(1.0)</code></td>
<td><code>!.value</code> or <code>1.0</code></td>
<td>Scalar constant</td>
</tr>
<tr>
<td><code>torch.full_like(x, value)</code></td>
<td><code>NTDSL.term ~fetch_op:(Constant value) ()</code></td>
<td>Shape-inferred</td>
</tr>
</tbody>
</table>
<h2 id="network-architecture-patterns">Network Architecture
Patterns</h2>
<h3 id="sequential-models">Sequential Models</h3>
<p><strong>PyTorch:</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">3</span>),</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    nn.MaxPool2d(<span class="dv">2</span>),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    nn.Flatten(),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">64</span><span class="op">*</span><span class="dv">14</span><span class="op">*</span><span class="dv">14</span>, <span class="dv">10</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>OCANNL:</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op model () =</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> conv1 = conv2d ~kernel_size:<span class="dv">3</span> () <span class="kw">in</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> pool = max_pool2d () <span class="kw">in</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">fun</span> x -&gt;</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> x = conv1 x <span class="kw">in</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> x = relu x <span class="kw">in</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> x = pool x <span class="kw">in</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">(* No flattening needed - FC layer works with spatial dims *)</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    { w_out } * x + { b_out = <span class="dv">0</span>.; o = [<span class="dv">10</span>] }</span></code></pre></div>
<h3 id="residual-connections">Residual Connections</h3>
<p><strong>PyTorch:</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResBlock(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, channels):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(channels, channels, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(channels)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(channels, channels, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(channels)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> F.relu(out)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.relu(out <span class="op">+</span> identity)</span></code></pre></div>
<p><strong>OCANNL:</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op resnet_block () =</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> conv1 = conv2d () <span class="kw">in</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> bn1 = batch_norm2d () <span class="kw">in</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> conv2 = conv2d () <span class="kw">in</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> bn2 = batch_norm2d () <span class="kw">in</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">fun</span> ~train_step x -&gt;</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> identity = x <span class="kw">in</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> out = conv1 x <span class="kw">in</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> out = bn1 ~train_step out <span class="kw">in</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> out = relu out <span class="kw">in</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> out = conv2 out <span class="kw">in</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> out = bn2 ~train_step out <span class="kw">in</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    relu (out + identity)</span></code></pre></div>
<h2 id="einsum-notation">Einsum Notation</h2>
<p>OCANNL’s einsum is more general than PyTorch’s, supporting row
variables and convolutions.</p>
<h3 id="syntax-modes">Syntax Modes</h3>
<p>OCANNL’s einsum has two syntax modes:</p>
<ol type="1">
<li><strong>Single-character mode</strong> (PyTorch-compatible):
<ul>
<li>Triggered when NO commas appear in the spec</li>
<li>Each alphanumeric character is an axis identifier</li>
<li>Spaces are optional and ignored: <code>&quot;ij&quot;</code> =
<code>&quot;i j&quot;</code></li>
</ul></li>
<li><strong>Multi-character mode</strong>:
<ul>
<li>Triggered by ANY comma in the spec</li>
<li>Trailing commas ignored</li>
<li>Identifiers can be multi-character (e.g., <code>height</code>,
<code>width</code>)</li>
<li>Must be separated by non-alphanumeric: <code>,</code> <code>|</code>
<code>-&gt;</code> <code>;</code> <code>=&gt;</code></li>
<li>Makes convolution syntax less confusing:
<code>stride*out+kernel</code></li>
</ul></li>
</ol>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 28%" />
<col style="width: 29%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr>
<th>Operation</th>
<th>PyTorch einsum</th>
<th>OCANNL single-char</th>
<th>OCANNL multi-char</th>
</tr>
</thead>
<tbody>
<tr>
<td>Matrix multiply</td>
<td><code>torch.einsum(&#39;ij,jk-&gt;ik&#39;, a, b)</code></td>
<td><code>a +* &quot;i j; j k =&gt; i k&quot; b</code></td>
<td><code>a +* &quot;i, j; j, k =&gt; i, k&quot; b</code></td>
</tr>
<tr>
<td>Batch matmul</td>
<td><code>torch.einsum(&#39;bij,bjk-&gt;bik&#39;, a, b)</code></td>
<td><code>a +* &quot;b i j; b j k =&gt; b i k&quot; b</code></td>
<td><code>a +* &quot;batch, i -&gt; j; batch, j -&gt; k =&gt; batch, i -&gt; k&quot; b</code></td>
</tr>
<tr>
<td>Attention scores</td>
<td><code>torch.einsum(&#39;bqhd,bkhd-&gt;bhqk&#39;, q, k)</code></td>
<td><code>q +* &quot;bq|hd; bk|hd =&gt; b|qk-&gt;h&quot; k</code></td>
<td><code>q +* &quot;b, q | h, d; b, k | h, d =&gt; b | q, k -&gt; h&quot; k</code></td>
</tr>
<tr>
<td>Convolution</td>
<td>N/A</td>
<td>better use multi-char</td>
<td><code>x +* &quot;... | stride*oh+kh, stride*ow+kw, ic; kh, kw, ic -&gt; oc =&gt; ... | oh, ow, oc&quot; kernel</code></td>
</tr>
</tbody>
</table>
<h3 id="row-variables">Row Variables</h3>
<ul>
<li><code>...</code> context-dependent ellipsis: expands to
<code>..batch..</code> in batch position, <code>..input..</code> before
<code>-&gt;</code>, <code>..output..</code> after
<code>-&gt;</code></li>
<li><code>..b..</code> for batch axes (arbitrary number)</li>
<li><code>..ic..</code>, <code>..oc..</code> for input/output channels
(can be multi-dimensional)</li>
<li><code>..spatial..</code> for spatial dimensions</li>
</ul>
<h2 id="common-gotchas-and-solutions">Common Gotchas and Solutions</h2>
<h3 id="variable-capture-with-einsum">Variable Capture with Einsum</h3>
<p>❌ <strong>Wrong:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> spec = <span class="st">&quot;... | h, w =&gt; ... | h0&quot;</span> <span class="kw">in</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x ++ spec [ <span class="st">&quot;h&quot;</span>; <span class="st">&quot;w&quot;</span> ]  <span class="co">(* Error: spec must be literal *)</span></span></code></pre></div>
<p>✅ <strong>Right:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>x ++ <span class="st">&quot;... | h, w =&gt; ... | h0&quot;</span> [ <span class="st">&quot;h&quot;</span>; <span class="st">&quot;w&quot;</span> ]</span></code></pre></div>
<h3 id="creating-non-learnable-constants">Creating Non-learnable
Constants</h3>
<p>❌ <strong>Wrong:</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>{ kernel = <span class="dv">1</span>. }  <span class="co">(* Creates learnable parameter *)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fl">1.0</span>             <span class="co">(* Creates fixed scalar shape *)</span></span></code></pre></div>
<p>✅ <strong>Right:</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.5</span> + <span class="fl">0.5</span>       <span class="co">(* Both are shape-inferred constant 1 *)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>NTDSL.term ~fetch_op:(Constant <span class="dv">1</span>.) ()</span></code></pre></div>
<h3 id="parameter-scoping">Parameter Scoping</h3>
<p>❌ <strong>Wrong:</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op network () x =</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">(* Sub-module defined after input *)</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> layer1 = my_layer () x <span class="kw">in</span>  </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  { global_param } + x </span></code></pre></div>
<p>✅ <strong>Right:</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op network () =</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">(* Sub-modules before input *)</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> layer1 = my_layer () <span class="kw">in</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">fun</span> x -&gt;</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">(* Inline definitions are lifted:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">       used here, but defined before layer1 *)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    { global_param } + layer1 x</span></code></pre></div>
<h3 id="flattening-for-linear-layers">Flattening for Linear Layers</h3>
<p>⚠️ <strong>Important:</strong> OCANNL doesn’t currently support
flattening/reshaping operations.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* This performs REDUCTION (sum), not flattening: *)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>x ++ <span class="st">&quot;... | ..spatial.. =&gt; ... | 0&quot;</span>  </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* OCANNL&#39;s approach: Let FC layers work with multiple axes!</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">   Instead of flattening [batch, h, w, c] to [batch, h*w*c],</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">   just let your FC layer handle [batch, h, w, c] directly.</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">   The matrix multiplication will work across all the axes. *)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">(* Example: FC layer after conv without flattening *)</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span>%op fc_after_conv () x =</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">(* x might have shape [batch, height, width, channels] *)</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  { w } * x + { b }  <span class="co">(* w will adapt to match x&#39;s shape *)</span></span></code></pre></div>
<h2 id="training-loop-patterns">Training Loop Patterns</h2>
<h3 id="basic-training-step">Basic Training Step</h3>
<p><strong>PyTorch:</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span></code></pre></div>
<p><strong>OCANNL (conceptual):</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* OCANNL handles training differently - see Train module *)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> sgd = Train.sgd_update ~learning_rate loss <span class="kw">in</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> train_step = Train.to_routine ~ctx [%cd update; sgd] <span class="kw">in</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Training happens via routines and contexts *)</span></span></code></pre></div>
<h2 id="debugging-tips">Debugging Tips</h2>
<h3 id="shape-errors">Shape Errors</h3>
<ul>
<li>Use <code>Shape.set_dim</code> (or <code>Shape.set_equal</code>) to
add constraints when inference needs hints</li>
<li>Remember that <code>..var..</code> row variables can match zero or
more axes</li>
<li>Check if you’re unnecessarily capturing variables in einsum</li>
</ul>
<h3 id="type-errors-with-inline-definitions">Type Errors with Inline
Definitions</h3>
<ul>
<li><code>{ x }</code> creates learnable parameters, not constants</li>
<li>Inline definitions are lifted to the unit parameter <code>()</code>
scope</li>
<li>Sub-modules don’t auto-lift - bind them before use</li>
</ul>
<h3 id="performance">Performance</h3>
<ul>
<li>Virtual tensors (like <code>0.5 + 0.5</code>) are inlined during
optimization</li>
<li>Row variables allow operations to work on grouped/multi-channel
data</li>
<li>Input axes (→) in kernels end up rightmost for better memory
locality</li>
</ul>
<h2 id="random-number-generation-details">Random Number Generation
Details</h2>
<h3 id="initialization-functions">Initialization Functions</h3>
<p>OCANNL’s random initialization has some important nuances:</p>
<ol type="1">
<li><p><strong>Default initialization is configurable</strong> - There
is a global reference that defaults to the <code>uniform</code>
operation but can be changed to any nullary operation.</p></li>
<li><p><strong>Divisibility requirements</strong> - Functions like
<code>uniform</code> require the total number of elements to be
divisible by certain values (they work with <code>uint4x32</code> for
efficiency):</p>
<ul>
<li><code>uniform()</code> - requires specific size divisibility for
efficient bit usage</li>
<li><code>uniform1()</code> - works pointwise on <code>uint4x32</code>
arrays, allows any size but wastes random bits</li>
</ul></li>
<li><p><strong>Deterministic PRNG</strong> - OCANNL uses counter-based
pseudo-random generation:</p>
<ul>
<li>Each <code>uniform()</code> call combines global seed with a unique
tensor identifier</li>
<li>Different calls generate different streams, but
deterministically</li>
<li>For training randomness (e.g., dropout), use <code>uniform_at</code>
with <code>~train_step</code> to split the randomness key</li>
</ul></li>
</ol>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">(* Parameter init - happens once, deterministic is fine *)</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>{ w = uniform () }</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">(* Training randomness - needs train_step for proper key splitting *)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>dropout ~rate:<span class="fl">0.5</span> () ~train_step x</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">(* internally uses: uniform_at !@train_step *)</span></span></code></pre></div>
<h2 id="further-resources">Further Resources</h2>
<ul>
<li><a href="../dev/neural_nets_lib/Ocannl/Shape/index.html">Shape
Inference Documentation</a> - Detailed einsum notation spec</li>
<li><a href="syntax_extensions.html">Syntax Extensions Guide</a> -
<code>%op</code> and <code>%cd</code> details<br />
</li>
<li><a href="https://github.com/ahrefs/ocannl/blob/master/lib/nn_blocks.ml">Neural
Network Blocks</a> - Example implementations</li>
<li><a href="https://github.com/ahrefs/ocannl/discussions">GitHub
Discussions</a> - Community Q&amp;A</li>
</ul>
</body>
</html>
