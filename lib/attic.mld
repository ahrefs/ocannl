{[
  let big_range = Array.init 300 ~f:(Int.to_float) in
  let r_data = TDSL.data ~label:"big_range" ~batch_dims:[2] ~output_dims:[3;5]
      (fun ~n:_ -> Init_op (Constant_fill big_range)) in
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 0.00e+0  1.00e+0  2.00e+0  3.00e+0  4.00e+0 │ 1.50e+1  1.60e+1  1.70e+1  1.80e+1  1.90e+1 ││
    ││      │ 5.00e+0  6.00e+0  7.00e+0  8.00e+0  9.00e+0 │ 2.00e+1  2.10e+1  2.20e+1  2.30e+1  2.40e+1 ││
    ││      │ 1.00e+1  1.10e+1  1.20e+1  1.30e+1  1.40e+1 │ 2.50e+1  2.60e+1  2.70e+1  2.80e+1  2.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 3.00e+1  3.10e+1  3.20e+1  3.30e+1  3.40e+1 │ 4.50e+1  4.60e+1  4.70e+1  4.80e+1  4.90e+1 ││
    ││      │ 3.50e+1  3.60e+1  3.70e+1  3.80e+1  3.90e+1 │ 5.00e+1  5.10e+1  5.20e+1  5.30e+1  5.40e+1 ││
    ││      │ 4.00e+1  4.10e+1  4.20e+1  4.30e+1  4.40e+1 │ 5.50e+1  5.60e+1  5.70e+1  5.80e+1  5.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 6.00e+1  6.10e+1  6.20e+1  6.30e+1  6.40e+1 │ 7.50e+1  7.60e+1  7.70e+1  7.80e+1  7.90e+1 ││
    ││      │ 6.50e+1  6.60e+1  6.70e+1  6.80e+1  6.90e+1 │ 8.00e+1  8.10e+1  8.20e+1  8.30e+1  8.40e+1 ││
    ││      │ 7.00e+1  7.10e+1  7.20e+1  7.30e+1  7.40e+1 │ 8.50e+1  8.60e+1  8.70e+1  8.80e+1  8.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
]}

Exec_as_cuda [~is_initial] and [~is_final]:
{[
    if is_initial then (
      if verbose then Stdio.printf "Exec_as_cuda.jit: copying host-to-device\n%!";
      List.iter arrays ~f:(function
        | ptr, { hosted = Some ndarray; global = Some name; global_ptr = Some (lazy dst); size_in_elems; _ }
          ->
            let tn = Hashtbl.find_exn traced_store ptr in
            if tn.read_before_write then (
              let f src = Cu.memcpy_H_to_D ~length:size_in_elems ~dst ~src () in
              if verbose && Utils.settings.with_debug then
                Stdio.printf "Exec_as_cuda.jit: memcpy_H_to_D for %s, length: %d\n%!" name size_in_elems;
              Ndarray.map { f } ndarray)
        | _ -> ()));


if is_final then (
      if verbose then Stdio.printf "Exec_as_cuda.jit: copying device-to-host\n%!";
      List.iter arrays ~f:(function
        | ptr, { hosted = Some ndarray; global = Some name; global_ptr = Some (lazy src); size_in_elems; _ }
          ->
            let tn = Hashtbl.find_exn traced_store ptr in
            if not tn.read_only then (
              let f dst = Cu.memcpy_D_to_H ~length:size_in_elems ~dst ~src () in
              if verbose && Utils.settings.with_debug then
                Stdio.printf "Exec_as_cuda.jit: memcpy_D_to_H for %s\n%!" name;
              Ndarray.map { f } ndarray)
        | _ -> ()));


  let finalizers =
    Array.of_list arrays
    |> Array.filter_map ~f:(fun (_, tn) ->
           match tn.mem with
           | Device_finally_host ->
               Option.map2 tn.local tn.global ~f:(fun l_name g_name ->
                   let b = Buffer.create 4096 in
                   let ppf = Stdlib.Format.formatter_of_buffer b in
                   let body idcs =
                     Low_level.Staged_compilation
                       (fun () ->
                         Stdlib.Format.fprintf ppf "@[<2>%s[%a] =@ %s[%a];@]" g_name pp_array_offset
                           (idcs, tn.dims) l_name pp_array_offset (idcs, tn.dims))
                   in
                   let loops = Low_level.loop_over_dims tn.dims ~body in
                   link ~traced_store ppf loops;
                   Stdlib.Format.pp_print_newline ppf ();
                   Buffer.contents b)
           | _ -> None)
  in
{|
  /* Finalization: copy local-to-global. */
  if (is_final) {
    %{String.concat_array ~sep:"\n    "
    @@ Array.map finalizers ~f:(String.substr_replace_all ~pattern:"\n" ~with_:"\n    ")}
  } 
|}
        
]}

Exec_as_gccjit [~is_initial] and [~is_final]:
{[
Option.iter hosted_ptr ~f:(fun hosted_ptr ->
            if is_local_finally_host mem then
              Block.eval finalize_block
              @@ RValue.call ctx (Function.builtin ctx "memcpy")
                   [
                     cast_void hosted_ptr;
                     cast_void @@ LValue.address @@ Option.value_exn local;
                     RValue.int ctx c_index size_in_bytes;
                   ]);

]}

Exec_as_cuda constants:
{[
  let constant_defs =
    List.filter_map arrays ~f:(fun (ptr, tn) ->
        match tn.mem with
        | Constant ->
            Option.map tn.global ~f:(fun t_name ->
                "__constant__ " ^ tn.num_typ ^ " " ^ t_name ^ "[" ^ Int.to_string tn.size_in_elems
                ^ if (Hashtbl.find_exn traced_store ptr).zero_initialized then "] = {0};" else "];")
        | _ -> None)
  in

{|
%{String.concat ~sep:"\n" constant_defs}
|}

        match mem with
        | Constant ->
            lazy
              (let ptr, size =
                 (* Defer till after compilation, to access the compiled-into module. *)
                 Cudajit.module_get_global
                   (Option.value_exn session_state.last_module)
                   ~name:(Tn.name v)
               in
               assert (Unsigned.Size_t.to_int size = size_in_bytes);
               ptr)
        | _ ->


]}

Ndarray:
{[
let get_as_int arr idx =
  let f x =
    let v = A.get x idx in
    try Float.to_int v
    with Invalid_argument _ ->
      Stdio.eprintf "\nRuntime error: Ndarray.get_as_int invalid float: %f\n%!" v;
      0
  in
  map { f } arr


let fold ~init ~f arr =
  let f arr = fold_bigarray ~init ~f arr in
  map { f } arr

]}

Shape:
{[

let rec scale ~num ~denom ?(force_conv = false) dim : dim =
  let ratio = Num.(num_of_int num // num_of_int denom) in
  let rat_to_int f n = Big_int.int_of_big_int @@ f @@ Ratio.normalize_ratio @@ Num.ratio_of_num n in
  let to_num = rat_to_int Ratio.numerator_ratio in
  let to_denom = rat_to_int Ratio.denominator_ratio in
  let dim_of_num ?label res =
    let num = to_num res and denom = to_denom res in
    let label =
      Option.map label ~f:(fun l ->
          let n = Int.to_string num in
          n ^ (if denom = 1 then "" else "/" ^ Int.to_string denom) ^ "*" ^ l)
    in
    get_dim ~d:(num / denom) ?label ()
  in
  let num = to_num ratio and denom = to_denom ratio in
  match dim with
  | Var _ -> Scaled { num; denom; dim }
  | Dim { d; label; proj_id = _ } ->
      let res = Num.(ratio */ num_of_int d) in
      if to_denom res = 1 || force_conv then dim_of_num ?label res else Scaled { num; denom; dim }
  | Scaled { num; denom; dim } ->
      let ratio = Num.(ratio */ num_of_int num // num_of_int denom) in
      let num = to_num ratio and denom = to_denom ratio in
      if force_conv then scale ~num ~denom ~force_conv dim else Scaled { num; denom; dim }


let indices_bio sh (type v) (arr : v array) =
  let n_batch = List.length sh.batch.dims in
  let batch : v Array.t = Array.sub arr ~pos:0 ~len:n_batch in
  let n_input = List.length sh.input.dims in
  let input = Array.sub arr ~pos:n_batch ~len:n_input in
  let n_output = List.length sh.output.dims in
  let output = Array.sub arr ~pos:(n_batch + n_input) ~len:n_output in
  (batch, input, output)

]}

C backend:
{[

let get_ptr ~(traced_store : Low_level.traced_store) ~ctx_nodes ~get_ident =
  let cache = Hashtbl.create (module Tn) in
  let mem = mem_properties traced_store in
  fun tn ->
    Hashtbl.find_or_add cache tn ~default:(fun () ->
        let mem = mem tn in
        let dims = Lazy.force tn.dims in
        let prec = tn.prec in
        let ident = get_ident tn in
        match (mem, ctx_nodes) with
        | From_context, Ctx_arrays ctx_arrays -> (
            match Map.find !ctx_arrays tn with
            | None ->
                let data =
                  Ndarray.create_array tn.Tn.prec ~dims
                  @@ Constant_fill { values = [| 0. |]; strict = false }
                in
                ctx_arrays := Map.add_exn !ctx_arrays ~key:tn ~data;
                get_c_ptr prec data
            | Some data -> get_c_ptr prec data)
        | From_context, Param_ptrs ptrs ->
            ptrs := (name, Param_ptr tn) :: !ptrs;
            ident
        | Constant_from_host, _ ->
            get_c_ptr prec @@ Option.value_exn ~here:[%here] @@ Lazy.force tn.array
        | Local_only, _ -> ident)


let mem_properties (traced_store : Low_level.traced_store) =
  let cache = Hashtbl.create (module Tn) in
  fun tn ->
    Hashtbl.find_or_add cache tn ~default:(fun () ->
        let is_on_host = Tn.is_hosted_force tn 33 in
        let is_materialized = Tn.is_materialized_force tn 331 in
        let is_constant = Tn.is_hosted_force ~specifically:Constant tn 332 in
        assert (Bool.(Option.is_some (Lazy.force tn.array) = is_on_host));
        let traced = Low_level.(get_node traced_store tn) in
        if not is_materialized then Local_only
        else if is_constant && traced.read_only then Constant_from_host
        else From_context)


  let run_variadic =
    [%log_level
      Nothing;
      let rec link :
            'a 'b 'idcs.
            'idcs Indexing.bindings ->
            (string * param_source) list ->
            ('a -> 'b) Ctypes.fn ->
            ('a -> 'b, 'idcs, 'p1, 'p2) Indexing.variadic =
       fun (type a b idcs) (binds : idcs Indexing.bindings) params (cs : (a -> b) Ctypes.fn) ->
        match (binds, params) with
        | Empty, [] -> Indexing.Result (Foreign.foreign ~from:code.result name cs)
        | Bind _, [] -> invalid_arg "Cc_backend.link: too few static index params"
        | Bind (_, bs), (_p_name, Static_idx _) :: ps ->
            Param_idx (ref 0, link bs ps Ctypes.(int @-> cs))
        | Empty, (p_name, Static_idx _) :: _ ->
            invalid_arg @@ "Cc_backend.link: too many static index params, found: " ^ p_name
        | bs, (_p_name, Log_file_name) :: ps ->
            Param_1 (ref (Some log_file_name), link bs ps Ctypes.(string @-> cs))
        | bs, (_p_name, Merge_buffer) :: ps ->
            let get_ptr (buffer, _) = Ndarray.get_voidptr_not_managed buffer in
            Param_2f (get_ptr, merge_buffer, link bs ps Ctypes.(ptr void @-> cs))
        | bs, (_p_name, Param_ptr tn) :: ps ->
            let nd = match Map.find arrays tn with Some nd -> nd | None -> assert false in
            let c_ptr = Ndarray.get_voidptr_not_managed nd in
            (* Stdlib.Printf.printf "DEBUG: %s param for %s is %s\n%!" _p_name (Tn.debug_name tn)
               (Ndarray.c_ptr_to_string nd); *)
            Param_2 (ref (Some c_ptr), link bs ps Ctypes.(ptr void @-> cs))
      in
      (* Reverse the input order because [Indexing.apply] will reverse it again. Important:
         [code.bindings] are traversed in the wrong order but that's OK because [link] only uses
         them to check the number of indices. *)
      let params = List.rev code.params in
      link code.bindings params Ctypes.(void @-> returning void)]
  in
]}

syntax_extensions.md:
{[
Local input here has a narrow meaning: a local input is a non-merge tensor node that should be computed by (other) assignments executed before the constructed sequence of assignments. Local inputs are tricky because they only appear in the bigger picture: they exclude recurrent nodes, which are only computed by the associated assignments, and nodes that need be copied from host to devices before they are used. They are unrelated (orthogonal) to merge nodes (which are external inputs). For tensors, outside the purview of `%cd` but to give context:

- The forward code has among its local inputs the child values nodes that were not forward roots at the time of tensor creation, and whose forward code was non-noop.
- The backprop code, if non-noop, has among its local inputs the grad node of the tensor itself.
]}

assignments.ml:
{[

(** Computes an approximation to {!field-embedded_nodes} where the passed assignments serve as
    {!field-asgns}. It is an over-approximation as it also includes recurrent nodes. It should
    ideally be called after relevant compilations, when {!Tnode.memory_mode} is known for the nodes. *)
let input_or_recurrent_nodes asgns =
  let open Utils.Set_O in
  let empty = Set.empty (module Tn) in
  let single = function
    | Node tn ->
        if Tn.known_constant tn || Tn.known_volatile tn || Tn.known_not_materialized tn then
          Set.empty (module Tn)
        else Set.singleton (module Tn) tn
    | Merge_buffer _ -> Set.empty (module Tn)
  in
  let maybe have lhs = if have then Set.singleton (module Tn) lhs else empty in
  let rec loop = function
    | Noop -> empty
    | Seq (t1, t2) -> loop t1 + (loop t2 - assigned t1)
    | Block_comment (_, t) -> loop t
    | Accum_binop { initialize_neutral; lhs; rhs1; rhs2; _ } ->
        maybe (not initialize_neutral) lhs + single rhs1 + single rhs2
    | Accum_unop { initialize_neutral; lhs; rhs; _ } ->
        maybe (not initialize_neutral) lhs + single rhs
    | Fetch _ -> empty
  and assigned = function
    | Noop -> Set.empty (module Tn)
    | Seq (t1, t2) -> assigned t1 + assigned t2
    | Block_comment (_, t) -> assigned t
    | Accum_binop { initialize_neutral; lhs; _ } -> maybe initialize_neutral lhs
    | Accum_unop { initialize_neutral; lhs; _ } -> maybe initialize_neutral lhs
    | Fetch { array; _ } -> Set.singleton (module Tn) array
  in
  loop asgns

]}

Old copying mechanism in backends.ml Multicore_backend:
{[

  let from_host (context : context) (tn : Tnode.t) =
    Option.value ~default:false
    @@ Option.map (Backend.get_buffer tn context.ctx) ~f:(fun c_arr ->
           match tn.Tnode.array with
           | (lazy (Some h_arr)) ->
               let%diagn_l_sexp work () =
                 Backend.host_to_buffer h_arr ~dst:c_arr;
                 [%diagn_sexp
                   [%log_block
                     "from_host " ^ Tnode.debug_name tn;
                     [%log "copied", Tnode.debug_name tn, "from host"];
                     [%log2_printbox
                       let indices =
                         Array.init (Array.length @@ Lazy.force tn.dims) ~f:(fun i -> i - 5)
                       in
                       Ndarray.render_array ~indices h_arr]]]
               in
               schedule_task context.stream
                 (Task.Task
                    {
                      context_lifetime = context;
                      description =
                        "from_host " ^ Tnode.debug_name tn ^ " dst " ^ context.stream.unique_name;
                      work;
                    });
               true
           | (lazy None) ->
               [%diagn_sexp
                 [%log_block
                   "nothing to copy from host";
                   [%log "for", Tnode.debug_name tn]]];
               false)

  let to_host (context : context) (tn : Tnode.t) =
    Option.value ~default:false
    @@ Option.map (Backend.get_buffer tn context.ctx) ~f:(fun c_arr ->
           match tn.Tnode.array with
           | (lazy (Some h_arr)) ->
               let%diagn_l_sexp work () =
                 Backend.buffer_to_host h_arr ~src:c_arr;
                 [%diagn_sexp
                   [%log_block
                     "to_host " ^ Tnode.debug_name tn;
                     [%log "copied to host"];
                     [%log2_printbox
                       let indices =
                         Array.init (Array.length @@ Lazy.force tn.dims) ~f:(fun i -> i - 5)
                       in
                       Ndarray.render_array ~indices h_arr]]]
               in
               schedule_task context.stream
                 (Task.Task
                    {
                      context_lifetime = context;
                      description =
                        "from_host " ^ Tnode.debug_name tn ^ " dst " ^ context.stream.unique_name;
                      work;
                    });
               true
           | (lazy None) ->
               [%diagn_sexp
                 [%log_block
                   "nothing to copy to host";
                   [%log "for", Tnode.debug_name tn]]];
               false)

  let device_to_device tn ~into_merge_buffer ~dst ~src =
    let dev = dst.stream in
    let schedule dst =
      let work =
        (* TODO: log the operation if [Utils.settings.with_log_level > 0]. *)
        match into_merge_buffer with
        | No -> fun () -> Backend.to_buffer tn ~dst ~src:src.ctx
        | Streaming ->
            fun () ->
              dev.merge_buffer :=
                Option.map ~f:(fun ptr -> (ptr, tn)) @@ Backend.get_buffer tn src.ctx
        | Copy ->
            fun () ->
              let size_in_bytes = Tnode.size_in_bytes tn in
              let allocated_capacity =
                Option.value ~default:0 @@ Option.map dev.allocated_buffer ~f:snd
              in
              if allocated_capacity < size_in_bytes then
                dev.allocated_buffer <-
                  Some
                    ( Backend.alloc_buffer ?old_buffer:dev.allocated_buffer ~size_in_bytes (),
                      size_in_bytes );
              let merge_ptr = fst @@ Option.value_exn dev.allocated_buffer in
              dev.merge_buffer := Some (merge_ptr, tn);
              Backend.to_buffer tn ~dst:merge_ptr ~src:src.ctx
      in
      let description =
        "device_to_device " ^ Tnode.debug_name tn ^ " dst " ^ dev.unique_name ^ " src "
        ^ src.stream.unique_name
      in
      schedule_task dev (Task.Task { context_lifetime = (src, dst); description; work })
    in
    match (Backend.get_buffer tn dst.ctx, Backend.get_buffer tn src.ctx) with
    | Some dst, Some _ ->
        schedule dst;
        true
    | _ -> false

]}