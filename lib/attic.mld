Specification of a terminal [Formula.t]'s shape. The [string] occurrences refer to [axis_labels]
    specs. Note: the specification is just a helper in constructing shapes.
{[type term_spec =
  | Unknown_shape
  (** The shape will need to be fully inferred. *)
  | Constant of {output_dims: int list; axis_labels: string}
  (** Shape of a constant has no batch nor input dimensions, only output dimensions. *)
  | Data of {batch_dims: int list; output_dims: int list; axis_labels: string}
  (** A data shape does not have input dimensions. *)
  | Params of {input_dims: int list; output_dims: int list; axis_labels: string}
  (** A parameters shape with fixed dimensionality. Parameters do not have batch dimensions. *)
  | Given_output_params of {input_dims: int list; output_dims: int list; axis_labels: string}
  (** A parameters shape with fixed output dimensionality, but input shape still gets inferred.
      Parameters do not have batch dimensions. *)
  | Transform of {batch_dims: int list; input_dims: int list; output_dims: int list; axis_labels: string}
  (** A non-differentiable transformation(s) shape. *)
  | Unknown_batch_data of {output_dims: int list; axis_labels: string}
  (** A data shape where the batch dimensions are left up to inference. *)
  | Deduced_params of deduce_dims
    (** Parameters with inferred dimensionality. Use cases:
        [Deduced_params Not_constrained] -- the shape will need to be fully inferred (no batch dims).
        [Deduced_params Input_equals_output] -- a hidden layer preserving the dimensionality.
        [Deduced_params (Input_output_scale 2.0)] -- an expansion hidden layer doubling the dimensionality.
        [Deduced_params (Input_output_scale 0.5)] -- an bottleneck hidden layer halving the dimensionality.
        Note that scalar axes (1D) are not scaled, for compatibility with broadcasting. *)

let of_term_spec id: term_spec -> t = function
  | Unknown_shape ->
    { batch=Unknown; input=Unknown; output=Unknown;
      axis_labels=Map.empty (module AxisKey);
      deduce_within_shape_constraints=Not_constrained; id }
  | Constant {output_dims; axis_labels} ->
    { batch=Given []; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Data {batch_dims; output_dims; axis_labels} ->
    { batch=Given batch_dims; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Params {input_dims; output_dims; axis_labels} ->
    { batch=Given []; input=Given input_dims; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Transform {batch_dims; input_dims; output_dims; axis_labels} ->
    { batch=Given batch_dims; input=Given input_dims; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Unknown_batch_data {output_dims; axis_labels} ->
    { batch=Unknown; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Deduced_params deduce_within_shape_constraints ->
    { batch=Given []; input=Unknown; output=Unknown;
      axis_labels=Map.empty (module AxisKey);
      deduce_within_shape_constraints; id }

let term_needs_gradient spec =
  match spec with
  | Unknown_shape -> true
  | Data _ -> false
  | Constant _ -> false
  | Params _ -> true
  | Given_output_arams _ -> true
  | Transform _ -> false
  | Unknown_batch_data _ -> false
  | Deduced_params _ -> true



  | Range_over_axis_from_end of int
  (** Fills in the index number of the specified axis counting from end.
      [Range_over_axis_from_end 1] is the range over the last axis. *)


  | Range_over_axis_from_end d ->
    init_array_of_prec prec dims ~f:(fun idcs -> Float.of_int @@ idcs.(Array.length idcs - d))

  | Init_op (Range_over_axis_from_end d) ->
    loop_bigarray arr ~f:(fun idcs -> cast @@ Float.of_int @@ idcs.(Array.length idcs - d))


  | Standard_gaussian
  (** Draws the values from N(0,1). *)


let fetch_bigarray (fetch_op: fetch_op) (type val_t b) (cast: float -> val_t)
    (_prec: (val_t, (val_t, b, Bigarray.c_layout) bigarray) precision)
    (arr: (val_t, b, Bigarray.c_layout) bigarray) =
  let dims = A.dims arr in
  match fetch_op with
  | Init_op (Unspecified) ->
    ()
  | Init_op (Constant_fill cs) ->
    let size = Array.length cs in
    let group_offset =
      Int.((global.session_step * Array.fold dims ~init:1 ~f:( * )) % size) in
    loop_bigarray arr
      ~f:(fun idcs -> cast cs.((indices_to_offset ~dims ~idcs + group_offset) % size))
  | Init_op (Range_over_offsets) ->
    loop_bigarray arr 
      ~f:(fun idcs -> cast @@ Float.of_int @@ indices_to_offset ~dims ~idcs)
  | Init_op (Standard_uniform) ->
    loop_bigarray arr ~f:(fun _ -> cast @@ Random.float_range 0.0 1.0)
  | Compute_point f ->
    loop_bigarray arr ~f:(fun idcs -> cast (f ~session_step:global.session_step ~dims ~idcs))

let fetch_ndarray fetch_op arr =
  let ff arr = fetch_bigarray fetch_op arr in
   cast_map_as_bigarray {ff} arr

let fetch_ndarray_callback ~op_or_id arr =
  let fetch_op =
    Either.value_map op_or_id ~first:Fn.id
      ~second:(Hashtbl.find_exn global.node_fetch_callbacks) in
  fetch_ndarray fetch_op arr


let init_callback ~op_or_id arr =
  let init_op =
    Either.value_map op_or_id ~first:Fn.id
      ~second:(Hashtbl.find_exn global.node_init_callbacks) in
  init init_op arr


  let point_input = FDSL.data ~label:"point_input" ~batch_dims:[1] ~output_dims:[2]
      (Compute_point (fun ~session_step:_ ~dims:_ ~idcs -> point.(idcs.(1)))) in

    let size = Array.length cs in
    let group_offset =
      Int.((global.session_step * Array.fold dims ~init:1 ~f:( * )) % size) in
    loop_bigarray arr
      ~f:(fun idcs -> cast cs.((indices_to_offset ~dims ~idcs + group_offset) % size))


type nulop =
  | Constant of float
  | Index_value of Shape.symbol option
  | Standard_uniform
[@@deriving sexp]


  let big_range = Array.init 300 ~f:(Int.to_float) in
  let r_data = FDSL.data ~label:"big_range" ~batch_dims:[2] ~output_dims:[3;5]
      (fun ~n:_ -> Init_op (Constant_fill big_range)) in
  refresh_session ();
  print_formula ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 0.00e+0  1.00e+0  2.00e+0  3.00e+0  4.00e+0 │ 1.50e+1  1.60e+1  1.70e+1  1.80e+1  1.90e+1 ││
    ││      │ 5.00e+0  6.00e+0  7.00e+0  8.00e+0  9.00e+0 │ 2.00e+1  2.10e+1  2.20e+1  2.30e+1  2.40e+1 ││
    ││      │ 1.00e+1  1.10e+1  1.20e+1  1.30e+1  1.40e+1 │ 2.50e+1  2.60e+1  2.70e+1  2.80e+1  2.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_formula ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 3.00e+1  3.10e+1  3.20e+1  3.30e+1  3.40e+1 │ 4.50e+1  4.60e+1  4.70e+1  4.80e+1  4.90e+1 ││
    ││      │ 3.50e+1  3.60e+1  3.70e+1  3.80e+1  3.90e+1 │ 5.00e+1  5.10e+1  5.20e+1  5.30e+1  5.40e+1 ││
    ││      │ 4.00e+1  4.10e+1  4.20e+1  4.30e+1  4.40e+1 │ 5.50e+1  5.60e+1  5.70e+1  5.80e+1  5.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_formula ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 6.00e+1  6.10e+1  6.20e+1  6.30e+1  6.40e+1 │ 7.50e+1  7.60e+1  7.70e+1  7.80e+1  7.90e+1 ││
    ││      │ 6.50e+1  6.60e+1  6.70e+1  6.80e+1  6.90e+1 │ 8.00e+1  8.10e+1  8.20e+1  8.30e+1  8.40e+1 ││
    ││      │ 7.00e+1  7.10e+1  7.20e+1  7.30e+1  7.40e+1 │ 8.50e+1  8.60e+1  8.70e+1  8.80e+1  8.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];




  (** Extract the values along the first batch axis, keeping the other axes fixed. *)
  let ( .:|{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:true

  (** Extract the values along the first input axis, keeping the other axes fixed. *)
  let ( .:/{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:true

  (** Extract the values along the first output axis, keeping the other axes fixed. *)
  let ( .:-{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:true

  (** Extract the values along the last batch axis, keeping the other axes fixed. *)
  let ( .|:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:false

  (** Extract the values along the last input axis, keeping the other axes fixed. *)
  let ( ./:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:false

  (** Extract the values along the last output axis, keeping the other axes fixed. *)
  let ( .-:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:false

  (** Set the values along the first batch axis, keeping the other axes fixed. *)
  let ( .:|{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:true

  (** Set the values along the first input axis, keeping the other axes fixed. *)
  let ( .:/{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:true

  (** Set the values along the first output axis, keeping the other axes fixed. *)
  let ( .:-{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:true

  (** Set the values along the last batch axis, keeping the other axes fixed. *)
  let ( .|:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:false

  (** Set the values along the last input axis, keeping the other axes fixed. *)
  let ( ./:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:false

  (** Set the values along the last output axis, keeping the other axes fixed. *)
  let ( .-:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:false




let get_value (type val_t arr_t) (prec : (val_t, arr_t) precision) uid : arr_t =
  let n = Hashtbl.find_exn global.node_store uid in
  match (prec, n.value) with
  | Byte_as_int, Byte_as_int_nd arr -> arr
  | Half_as_int, Half_as_int_nd arr -> arr
  | Single, Single_nd arr -> arr
  | Double, Double_nd arr -> arr
  | _, arr ->
      raise
      @@ Runtime_error
           ( "Precision mismatch: expected " ^ precision_to_string prec ^ ", got "
             ^ ndarray_precision_to_string arr,
             Some n )

let get_grad (type val_t arr_t) (prec : (val_t, arr_t) precision) uid : arr_t =
  let n = Hashtbl.find_exn global.node_store uid in
  match (prec, n.grad) with
  | Byte_as_int, Some (Byte_as_int_nd arr) -> arr
  | Half_as_int, Some (Half_as_int_nd arr) -> arr
  | Single, Some (Single_nd arr) -> arr
  | Double, Some (Double_nd arr) -> arr
  | _, Some arr ->
      raise
      @@ Runtime_error
           ( "Precision mismatch: expected " ^ precision_to_string prec ^ ", got "
             ^ ndarray_precision_to_string arr,
             Some n )
  | _, None -> raise @@ Runtime_error ("get_grad: non-form node", Some n)


    let nontask_ts, task_ts = partition_tf_with_comment ~f:parallelizable t_c_s in
    let nontask_ts = rebalance @@ Array.map nontask_ts ~f:(fun (t, _) -> loop ~single_task:true t) in
    let task_ts = Lines (Array.map ~f:snd task_ts) in
    match s with
    | None -> Lines (flat_lines [| nontask_ts; task_ts |])
    | Some s -> Lines (flat_lines [| Comment s; nontask_ts; task_ts |])





  mutable localized_to : int option;
      (** The ID of the task to which the tensor is localized. A non-none value by itself does not guarantee
          that all of the tensor's computations are localized to a single task, only that those which are
          only use the given task. *)
  mutable read_by_localized : int list;
      (** Tasks from which this tensor is read by localized computations. *)
  mutable debug_read_by_localized : string list;


let localize_tensors store ~for_task_id llc =
  let for_task = Some for_task_id in
  let debug = ref "" in
  let rec loop = function
    | Lines llcs -> Array.iter ~f:loop llcs
    | For_loop { body; _ } | Dynamic_indices { body; _ } -> loop body
    | Rebalance (_, cs) -> Array.iter ~f:loop cs
    | Set (ptr, _, llv) ->
        let n = Node.get ptr.id in
        if Option.is_some n.localized_to then assert ([%equal: int option] n.localized_to for_task)
        else n.localized_to <- Some for_task_id;
        let never_device_only =
          match ptr.field with
          | Node.Value -> n.value_never_device_only
          | Node.Grad -> n.grad_never_device_only
        in
        if never_device_only then (
          debug := Node.tensor_ptr_name ptr;
          loop_float llv)
    | Set_local (_, llv) -> loop_float llv
    | Comment _ -> ()
    | If_task_id_is { for_task_id = id2; body; _ } ->
        assert (id2 = for_task_id);
        loop body
  and loop_float = function
    | Local_scope { body; _ } -> loop body
    | Get_local _ | Get_global _ -> ()
    | Get (ptr, _) ->
        let n = Node.get ptr.id in
        let dn = get_node store ptr in
        dn.non_device_only <- true;
        n.read_by_localized <- for_task_id :: n.read_by_localized;
        n.debug_read_by_localized <- !debug :: n.debug_read_by_localized
    | Constant _ -> ()
    | Binop (_, v1, v2) ->
        loop_float v1;
        loop_float v2
    | Unop (_, v) -> loop_float v
  in
  loop llc


let rebalance_across_tasks = ref true

let rebalance store llcs =
  if not !rebalance_across_tasks then (
    let for_task_id = 0 in
    let body = Lines (flat_lines llcs) in
    localize_tensors store ~for_task_id body;
    If_task_id_is { for_task_id; body })
  else
    let tasks = Array.create ~len:!Shape.num_parallel_tasks [] in
    Array.iteri llcs ~f:(fun task body ->
        let i = task % !Shape.num_parallel_tasks in
        tasks.(i) <- body :: tasks.(i));
    Lines
      (Array.map tasks ~f:(Fn.compose flat_lines Array.of_list_rev)
      |> Array.mapi ~f:(fun for_task_id lines ->
             let body = Lines (flat_lines lines) in
             If_task_id_is { for_task_id; body }))


(* Inside cleanup_virtual_llc: *)
    | Rebalance (s, cs) when balanced -> (
        let cs = flat_lines @@ Array.filter_map cs ~f:loop in
        match (s, cs) with
        | _, [||] -> None
        | None, [| c |] -> Some c
        | _, cs ->
            let c = Array.map ~f:(fun s -> Comment s) @@ Option.to_array s in
            Some (Lines (Array.append c cs)))
    | Rebalance (s, cs) -> (
        (* Don't flatten lines before rebalancing: keep elements of [cs] as single units. *)
        let multitask, unitask = partition_tf_with_comment ~f:has_parallel_dim cs in
        let rebalanced =
          let unitask = Array.filter_map unitask ~f:(loop_proc ~balanced:true ~env_dom) in
          if Array.is_empty unitask then None else Some (rebalance traced_store unitask)
        in
        let multitask = flat_lines @@ Array.filter_map ~f:loop multitask in
        if Array.is_empty multitask && Option.is_none rebalanced then None
        else
          match s with
          | None -> Some (Lines (Array.append (Option.to_array rebalanced) multitask))
          | Some s ->
              Some
                (Lines (flat_lines @@ Array.concat [ [| Comment s |]; Option.to_array rebalanced; multitask ]))
        )

Set (ptr, idcs, (Binop (op, Get (ptr2, idcs2), v2) as v))
      when Node.equal_tensor_ptr ptr ptr2 && [%equal: Shape.axis_index array] idcs idcs2 ->
        let tensor = get_tensor ~traced_store ~jit_code:(pp_ll ~dyn_env) ~dyn_env ~idcs ptr in
        let old_locals = !locals in
        let num_typ = tensor.num_typ and is_double = tensor.is_double in
        let loop_f = pp_float ~dyn_env ~num_typ ~is_double in
        let num_closing_braces = pp_top_locals ~dyn_env ppf v2 in
        if
          List.exists ~f:(equal_sync_properties tensor.sync)
            [ Update_globally_for_thread; Update_globally_for_block ]
        then (
          (* Because of SIMD-like computation over warps, updates must be atomic. *)
          if Code.equal_binop op Add then
            fprintf ppf "@[<2>atomicAdd@[<2>(%s + %a,@ %a@])" (Option.value_exn tensor.global) (pp_array_offset Global)
              (idcs, tensor.dims) loop_f v2
          else failwith @@ "Exec_as_cuda: atomic updates only implemented for addition: " ^ Sexp.to_string_hum ([%sexp_of: unit_low_level] llc) (*
            fprintf ppf "@[<2>%s[%a] =@ " (Option.value_exn tensor.global) (pp_array_offset Global)
              (idcs, tensor.dims);
            loop_binop ~num_typ ~is_double ppf op
              (fun ppf () ->
                fprintf ppf "%s[%a]" (Option.value_exn tensor.global) (pp_array_offset Global)
                  (idcs, tensor.dims))
              () loop_f v2 *);
          fprintf ppf ";@;<1 -2>%s[%a] =@ %s[%a];@]" (Option.value_exn tensor.local)
            (pp_array_offset tensor.run_scope) (idcs, tensor.dims) (Option.value_exn tensor.global)
            (pp_array_offset Global) (idcs, tensor.dims))
        else
          fprintf ppf "@[<2>%a[@,%a] =@ %a;@]" pp_get_run_ptr tensor (pp_array_offset tensor.run_scope)
            (idcs, tensor.dims) loop_f v;
        for _ = 1 to num_closing_braces do
          fprintf ppf "@]@ }@,"
        done;
        locals := old_locals
    

]}
