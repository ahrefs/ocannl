Specification of a terminal [Tensor.t]'s shape. The [string] occurrences refer to [axis_labels]
    specs. Note: the specification is just a helper in constructing shapes.
{[type term_spec =
  | Unknown_shape
  (** The shape will need to be fully inferred. *)
  | Constant of {output_dims: int list; axis_labels: string}
  (** Shape of a constant has no batch nor input dimensions, only output dimensions. *)
  | Data of {batch_dims: int list; output_dims: int list; axis_labels: string}
  (** A data shape does not have input dimensions. *)
  | Params of {input_dims: int list; output_dims: int list; axis_labels: string}
  (** A parameters shape with fixed dimensionality. Parameters do not have batch dimensions. *)
  | Given_output_params of {input_dims: int list; output_dims: int list; axis_labels: string}
  (** A parameters shape with fixed output dimensionality, but input shape still gets inferred.
      Parameters do not have batch dimensions. *)
  | Transform of {batch_dims: int list; input_dims: int list; output_dims: int list; axis_labels: string}
  (** A non-differentiable transformation(s) shape. *)
  | Unknown_batch_data of {output_dims: int list; axis_labels: string}
  (** A data shape where the batch dimensions are left up to inference. *)
  | Deduced_params of deduce_dims
    (** Parameters with inferred dimensionality. Use cases:
        [Deduced_params Not_constrained] -- the shape will need to be fully inferred (no batch dims).
        [Deduced_params Input_equals_output] -- a hidden layer preserving the dimensionality.
        [Deduced_params (Input_output_scale 2.0)] -- an expansion hidden layer doubling the dimensionality.
        [Deduced_params (Input_output_scale 0.5)] -- an bottleneck hidden layer halving the dimensionality.
        Note that scalar axes (1D) are not scaled, for compatibility with broadcasting. *)

let of_term_spec id: term_spec -> t = function
  | Unknown_shape ->
    { batch=Unknown; input=Unknown; output=Unknown;
      axis_labels=Map.empty (module AxisKey);
      deduce_within_shape_constraints=Not_constrained; id }
  | Constant {output_dims; axis_labels} ->
    { batch=Given []; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Data {batch_dims; output_dims; axis_labels} ->
    { batch=Given batch_dims; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Params {input_dims; output_dims; axis_labels} ->
    { batch=Given []; input=Given input_dims; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Transform {batch_dims; input_dims; output_dims; axis_labels} ->
    { batch=Given batch_dims; input=Given input_dims; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Unknown_batch_data {output_dims; axis_labels} ->
    { batch=Unknown; input=Given []; output=Given output_dims;
      axis_labels=(axis_labels_of_spec axis_labels).labels;
      deduce_within_shape_constraints=Not_constrained; id }
  | Deduced_params deduce_within_shape_constraints ->
    { batch=Given []; input=Unknown; output=Unknown;
      axis_labels=Map.empty (module AxisKey);
      deduce_within_shape_constraints; id }

let term_needs_gradient spec =
  match spec with
  | Unknown_shape -> true
  | Data _ -> false
  | Constant _ -> false
  | Params _ -> true
  | Given_output_arams _ -> true
  | Transform _ -> false
  | Unknown_batch_data _ -> false
  | Deduced_params _ -> true



  | Range_over_axis_from_end of int
  (** Fills in the index number of the specified axis counting from end.
      [Range_over_axis_from_end 1] is the range over the last axis. *)


  | Range_over_axis_from_end d ->
    init_array_of_prec prec dims ~f:(fun idcs -> Float.of_int @@ idcs.(Array.length idcs - d))

  | Init_op (Range_over_axis_from_end d) ->
    loop_bigarray arr ~f:(fun idcs -> cast @@ Float.of_int @@ idcs.(Array.length idcs - d))


  | Standard_gaussian
  (** Draws the values from N(0,1). *)


let fetch_bigarray (fetch_op: fetch_op) (type val_t b) (cast: float -> val_t)
    (_prec: (val_t, (val_t, b, Bigarray.c_layout) bigarray) precision)
    (arr: (val_t, b, Bigarray.c_layout) bigarray) =
  let dims = A.dims arr in
  match fetch_op with
  | Init_op (Unspecified) ->
    ()
  | Init_op (Constant_fill cs) ->
    let size = Array.length cs in
    let group_offset =
      Int.((global.session_step * Array.fold dims ~init:1 ~f:( * )) % size) in
    loop_bigarray arr
      ~f:(fun idcs -> cast cs.((indices_to_offset ~dims ~idcs + group_offset) % size))
  | Init_op (Range_over_offsets) ->
    loop_bigarray arr 
      ~f:(fun idcs -> cast @@ Float.of_int @@ indices_to_offset ~dims ~idcs)
  | Init_op (Standard_uniform) ->
    loop_bigarray arr ~f:(fun _ -> cast @@ Random.float_range 0.0 1.0)
  | Compute_point f ->
    loop_bigarray arr ~f:(fun idcs -> cast (f ~session_step:global.session_step ~dims ~idcs))

let fetch_ndarray fetch_op arr =
  let ff arr = fetch_bigarray fetch_op arr in
   cast_map_as_bigarray {ff} arr

let fetch_ndarray_callback ~op_or_id arr =
  let fetch_op =
    Either.value_map op_or_id ~first:Fn.id
      ~second:(Hashtbl.find_exn global.node_fetch_callbacks) in
  fetch_ndarray fetch_op arr


let init_callback ~op_or_id arr =
  let init_op =
    Either.value_map op_or_id ~first:Fn.id
      ~second:(Hashtbl.find_exn global.node_init_callbacks) in
  init init_op arr


  let point_input = TDSL.data ~label:"point_input" ~batch_dims:[1] ~output_dims:[2]
      (Compute_point (fun ~session_step:_ ~dims:_ ~idcs -> point.(idcs.(1)))) in

    let size = Array.length cs in
    let group_offset =
      Int.((global.session_step * Array.fold dims ~init:1 ~f:( * )) % size) in
    loop_bigarray arr
      ~f:(fun idcs -> cast cs.((indices_to_offset ~dims ~idcs + group_offset) % size))


type nulop =
  | Constant of float
  | Index_value of Shape.symbol option
  | Standard_uniform
[@@deriving sexp]


  let big_range = Array.init 300 ~f:(Int.to_float) in
  let r_data = TDSL.data ~label:"big_range" ~batch_dims:[2] ~output_dims:[3;5]
      (fun ~n:_ -> Init_op (Constant_fill big_range)) in
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 0.00e+0  1.00e+0  2.00e+0  3.00e+0  4.00e+0 │ 1.50e+1  1.60e+1  1.70e+1  1.80e+1  1.90e+1 ││
    ││      │ 5.00e+0  6.00e+0  7.00e+0  8.00e+0  9.00e+0 │ 2.00e+1  2.10e+1  2.20e+1  2.30e+1  2.40e+1 ││
    ││      │ 1.00e+1  1.10e+1  1.20e+1  1.30e+1  1.40e+1 │ 2.50e+1  2.60e+1  2.70e+1  2.80e+1  2.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 3.00e+1  3.10e+1  3.20e+1  3.30e+1  3.40e+1 │ 4.50e+1  4.60e+1  4.70e+1  4.80e+1  4.90e+1 ││
    ││      │ 3.50e+1  3.60e+1  3.70e+1  3.80e+1  3.90e+1 │ 5.00e+1  5.10e+1  5.20e+1  5.30e+1  5.40e+1 ││
    ││      │ 4.00e+1  4.10e+1  4.20e+1  4.30e+1  4.40e+1 │ 5.50e+1  5.60e+1  5.70e+1  5.80e+1  5.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 6.00e+1  6.10e+1  6.20e+1  6.30e+1  6.40e+1 │ 7.50e+1  7.60e+1  7.70e+1  7.80e+1  7.90e+1 ││
    ││      │ 6.50e+1  6.60e+1  6.70e+1  6.80e+1  6.90e+1 │ 8.00e+1  8.10e+1  8.20e+1  8.30e+1  8.40e+1 ││
    ││      │ 7.00e+1  7.10e+1  7.20e+1  7.30e+1  7.40e+1 │ 8.50e+1  8.60e+1  8.70e+1  8.80e+1  8.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];




  (** Extract the values along the first batch axis, keeping the other axes fixed. *)
  let ( .:|{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:true

  (** Extract the values along the first input axis, keeping the other axes fixed. *)
  let ( .:/{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:true

  (** Extract the values along the first output axis, keeping the other axes fixed. *)
  let ( .:-{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:true

  (** Extract the values along the last batch axis, keeping the other axes fixed. *)
  let ( .|:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:false

  (** Extract the values along the last input axis, keeping the other axes fixed. *)
  let ( ./:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:false

  (** Extract the values along the last output axis, keeping the other axes fixed. *)
  let ( .-:{} ) = get_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:false

  (** Set the values along the first batch axis, keeping the other axes fixed. *)
  let ( .:|{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:true

  (** Set the values along the first input axis, keeping the other axes fixed. *)
  let ( .:/{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:true

  (** Set the values along the first output axis, keeping the other axes fixed. *)
  let ( .:-{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:true

  (** Set the values along the last batch axis, keeping the other axes fixed. *)
  let ( .|:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Batch ~from_left:false

  (** Set the values along the last input axis, keeping the other axes fixed. *)
  let ( ./:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Input ~from_left:false

  (** Set the values along the last output axis, keeping the other axes fixed. *)
  let ( .-:{}<- ) = set_value_along_axis ~over_kind:Shape.AxisKey.Output ~from_left:false




let get_value (type val_t arr_t) (prec : (val_t, arr_t) precision) uid : arr_t =
  let n = Hashtbl.find_exn global.node_store uid in
  match (prec, n.value) with
  | Byte_as_int, Byte_as_int_nd arr -> arr
  | Half_as_int, Half_as_int_nd arr -> arr
  | Single, Single_nd arr -> arr
  | Double, Double_nd arr -> arr
  | _, arr ->
      raise
      @@ Runtime_error
           ( "Precision mismatch: expected " ^ precision_to_string prec ^ ", got "
             ^ ndarray_precision_to_string arr,
             Some n )

let get_grad (type val_t arr_t) (prec : (val_t, arr_t) precision) uid : arr_t =
  let n = Hashtbl.find_exn global.node_store uid in
  match (prec, n.grad) with
  | Byte_as_int, Some (Byte_as_int_nd arr) -> arr
  | Half_as_int, Some (Half_as_int_nd arr) -> arr
  | Single, Some (Single_nd arr) -> arr
  | Double, Some (Double_nd arr) -> arr
  | _, Some arr ->
      raise
      @@ Runtime_error
           ( "Precision mismatch: expected " ^ precision_to_string prec ^ ", got "
             ^ ndarray_precision_to_string arr,
             Some n )
  | _, None -> raise @@ Runtime_error ("get_grad: non-diff node", Some n)


    let nontask_ts, task_ts = partition_tf_with_comment ~f:parallelizable t_c_s in
    let nontask_ts = rebalance @@ Array.map nontask_ts ~f:(fun (t, _) -> loop ~single_task:true t) in
    let task_ts = Lines (Array.map ~f:snd task_ts) in
    match s with
    | None -> Lines (flat_lines [| nontask_ts; task_ts |])
    | Some s -> Lines (flat_lines [| Comment s; nontask_ts; task_ts |])





  mutable localized_to : int option;
      (** The ID of the task to which the tensor is localized. A non-none value by itself does not guarantee
          that all of the tensor's computations are localized to a single task, only that those which are
          only use the given task. *)
  mutable read_by_localized : int list;
      (** Tasks from which this tensor is read by localized computations. *)
  mutable debug_read_by_localized : string list;


let localize_tensors store ~for_task_id llc =
  let for_task = Some for_task_id in
  let debug = ref "" in
  let rec loop = function
    | Lines llcs -> Array.iter ~f:loop llcs
    | For_loop { body; _ } | Dynamic_indices { body; _ } -> loop body
    | Rebalance (_, cs) -> Array.iter ~f:loop cs
    | Set (ptr, _, llv) ->
        let n = Node.get ptr.id in
        if Option.is_some n.localized_to then assert ([%equal: int option] n.localized_to for_task)
        else n.localized_to <- Some for_task_id;
        let never_device_only =
          match ptr.field with
          | Node.Value -> n.value_never_device_only
          | Node.Grad -> n.grad_never_device_only
        in
        if never_device_only then (
          debug := Ndarray.ptr_name ptr;
          loop_float llv)
    | Set_local (_, llv) -> loop_float llv
    | Comment _ -> ()
  and loop_float = function
    | Local_scope { body; _ } -> loop body
    | Get_local _ | Get_global _ -> ()
    | Get (ptr, _) ->
        let n = Node.get ptr.id in
        let dn = get_node store ptr in
        dn.non_device_only <- true;
        n.read_by_localized <- for_task_id :: n.read_by_localized;
        n.debug_read_by_localized <- !debug :: n.debug_read_by_localized
    | Constant _ -> ()
    | Binop (_, v1, v2) ->
        loop_float v1;
        loop_float v2
    | Unop (_, v) -> loop_float v
  in
  loop llc


let rebalance_across_tasks = ref true

let rebalance store llcs =
  if not !rebalance_across_tasks then (
    let for_task_id = 0 in
    let body = Lines (flat_lines llcs) in
    localize_tensors store ~for_task_id body;
    If_task_id_is { for_task_id; body })
  else
    let tasks = Array.create ~len:!Shape.num_parallel_tasks [] in
    Array.iteri llcs ~f:(fun task body ->
        let i = task % !Shape.num_parallel_tasks in
        tasks.(i) <- body :: tasks.(i));
    Lines
      (Array.map tasks ~f:(Fn.compose flat_lines Array.of_list_rev)
      |> Array.mapi ~f:(fun for_task_id lines ->
             let body = Lines (flat_lines lines) in
             If_task_id_is { for_task_id; body }))


(* Inside cleanup_virtual_llc: *)
    | Rebalance (s, cs) when balanced -> (
        let cs = flat_lines @@ Array.filter_map cs ~f:loop in
        match (s, cs) with
        | _, [||] -> None
        | None, [| c |] -> Some c
        | _, cs ->
            let c = Array.map ~f:(fun s -> Comment s) @@ Option.to_array s in
            Some (Lines (Array.append c cs)))
    | Rebalance (s, cs) -> (
        (* Don't flatten lines before rebalancing: keep elements of [cs] as single units. *)
        let multitask, unitask = partition_tf_with_comment ~f:has_parallel_dim cs in
        let rebalanced =
          let unitask = Array.filter_map unitask ~f:(loop_proc ~balanced:true ~env_dom) in
          if Array.is_empty unitask then None else Some (rebalance traced_store unitask)
        in
        let multitask = flat_lines @@ Array.filter_map ~f:loop multitask in
        if Array.is_empty multitask && Option.is_none rebalanced then None
        else
          match s with
          | None -> Some (Lines (Array.append (Option.to_array rebalanced) multitask))
          | Some s ->
              Some
                (Lines (flat_lines @@ Array.concat [ [| Comment s |]; Option.to_array rebalanced; multitask ]))
        )

Set (ptr, idcs, (Binop (op, Get (ptr2, idcs2), v2) as v))
      when Nd.equal_ptr ptr ptr2 && [%equal: Shape.axis_index array] idcs idcs2 ->
        let tensor = get_tensor ~traced_store ~jit_code:(pp_ll ~dyn_env) ~dyn_env ~idcs ptr in
        let old_locals = !locals in
        let num_typ = tensor.num_typ and is_double = tensor.is_double in
        let loop_f = pp_float ~dyn_env ~num_typ ~is_double in
        let num_closing_braces = pp_top_locals ~dyn_env ppf v2 in
        if
          List.exists ~f:(equal_sync_properties tensor.sync)
            [ Update_globally_for_thread; Update_globally_for_block ]
        then (
          (* Because of SIMD-like computation over warps, updates must be atomic. *)
          if Code.equal_binop op Add then
            fprintf ppf "@[<2>atomicAdd@[<2>(%s + %a,@ %a@])" (Option.value_exn tensor.global) (pp_array_offset Global)
              (idcs, tensor.dims) loop_f v2
          else failwith @@ "Exec_as_cuda: atomic updates only implemented for addition: " ^ Sexp.to_string_hum ([%sexp_of: Low_level.t] llc) (*
            fprintf ppf "@[<2>%s[%a] =@ " (Option.value_exn tensor.global) (pp_array_offset Global)
              (idcs, tensor.dims);
            loop_binop ~num_typ ~is_double ppf op
              (fun ppf () ->
                fprintf ppf "%s[%a]" (Option.value_exn tensor.global) (pp_array_offset Global)
                  (idcs, tensor.dims))
              () loop_f v2 *);
          fprintf ppf ";@;<1 -2>%s[%a] =@ %s[%a];@]" (Option.value_exn tensor.local)
            (pp_array_offset tensor.run_scope) (idcs, tensor.dims) (Option.value_exn tensor.global)
            (pp_array_offset Global) (idcs, tensor.dims))
        else
          fprintf ppf "@[<2>%a[@,%a] =@ %a;@]" pp_get_run_ptr tensor (pp_array_offset tensor.run_scope)
            (idcs, tensor.dims) loop_f v;
        for _ = 1 to num_closing_braces do
          fprintf ppf "@]@ }@,"
        done;
        locals := old_locals
    
(* Conditional on task id for Exec_as_gccjit: *)
 | If_task_id_is { for_task_id; body } -> (
        match state.task_id with
        | Some task_id ->
            let open Gccjit in
            let id = get_uid () in
            let b_if_body =
              Block.create ~name:("body_if_task_id_is_" ^ Int.to_string for_task_id ^ "_" ^ id) func
            in
            let b_after_if =
              Block.create ~name:("after_if_task_id_is_" ^ Int.to_string for_task_id ^ "_" ^ id) func
            in
            let guard =
              RValue.comparison ctx Eq (RValue.param task_id) (RValue.int ctx c_index for_task_id)
            in
            Block.cond_jump !current_block guard b_if_body (* on true *) b_after_if (* on false *);
            current_block := b_if_body;
            loop ~name body;
            Block.jump !current_block b_after_if;
            current_block := b_after_if
        | None -> loop ~name body)




type annot = {
  mutable never_virtual : bool;
  mutable never_device_only : bool;
  mutable backend_info : string;
  shape : Shape.t;
}
[@@deriving sexp]

let annot shape =
  {
    value_never_virtual = false;
    value_never_device_only = false;
    grad_never_virtual = false;
    grad_never_device_only = false;
    value_distributes_over_sum = false;
    backend_info = "";
    shape;
  }

let update_shape local_update =
  let n = get local_update.Shape.shape.id in
  assert (phys_equal local_update.Shape.shape n.Node.annot.shape);
  Shape.propagate_shapes local_update;
  n.Node.axis_labels := Shape.axis_map_to_dims_index @@ local_update.shape.axis_labels;
  n.Node.default_display_indices := Shape.default_display_indices local_update.shape


(*
type int_env = int env

let sexp_of_int_env env =
  [%sexp_of: (sym_index * int) list * (Indexing.symbol * int) list] (Map.to_alist env, Map.to_alist dyn_env)
*)

let set_from_float ptr idcs value = Nd.set_from_float (Option.value_exn @@ get_tensor ptr) idcs value
let fill_from_float ptr value = Nd.fill_from_float (Option.value_exn @@ get_tensor ptr) value
let get_as_float ptr idcs = Nd.get_as_float (Option.value_exn @@ get_tensor ptr) idcs
let debug_verbose_trace = ref false



let interpret_code llc =
  (* Local scope ids can be non-unique due to inlining. *)
  let locals = ref Map.Poly.empty in
  let lookup ?provider_dim env indices =
    try
      Array.map indices ~f:(function
        | Indexing.Fixed_idx i -> i
        | Iterator it -> Map.find_exn env it
        | Dynamic_recipient s -> Map.find_exn env s
        | Frozen_recipient s -> Map.find_exn env s
        | Dynamic_provider _ -> Option.value_exn provider_dim)
    with Caml.Not_found | Not_found_s _ ->
      if !debug_verbose_trace then
        Caml.Format.printf "TRACE: lookup error for env keys=@ %a\n%!" Sexp.pp_hum
          ([%sexp_of: Indexing.symbol list] @@ Map.keys env);
      failwith "interpret_code: index lookup error, set CDSL.debug_verbose_trace for details"
  in
  let rec loop_proc env llc : unit =
    let loop = loop_proc env in
    match (llc : t) with
    | (Lines body : t) -> Array.iter ~f:loop body
    | (For_loop { index; from_; to_; body; trace_it = _ } : t) ->
        for data = from_ to to_ do
          (* We could allow replacement, but in fact we do not want repeated indices because backends
             like Gccjit require unique indices (for convenience). *)
          loop_proc (Map.add_exn env ~key:index ~data) body
        done
    | Rebalance (_, cs) ->
        (* FIXME: NOT IMPLEMENTED YET *)
        Array.iter ~f:loop cs
    | Zero_out ptr -> Ndarray.fill_from_float (Option.value_exn @@ get_tensor ptr) 0.0
    | Set (ptr, indices, Binop (op, Get (ptr2, indices2), c2))
      when Nd.equal_ptr ptr ptr2 && [%equal: Indexing.axis_index array] indices indices2 ->
        if !debug_verbose_trace then
          Caml.Format.printf "{TRACE: update %a [%a] <- ...\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices);
        let idcs = lookup env indices in
        (* Order computation to reduce prevalence of race conditions. *)
        let v2 = loop_float env c2 in
        let v1 =
          try get_as_float ptr idcs
          with e ->
            Caml.Format.printf "ERROR: %a [%a -> %a] -- indices out of bounds\n%!" Sexp.pp_hum
              ([%sexp_of: Ndarray.ptr] ptr)
              Sexp.pp_hum
              ([%sexp_of: Indexing.axis_index array] indices)
              Sexp.pp_hum
              ([%sexp_of: int array] idcs);
            Node.print_node_preamble @@ get ptr.id;
            raise e
        in
        let result = interpret_binop op v1 v2 in
        if !debug_verbose_trace then
          Caml.Format.printf "TRACE: %a [%a -> %a] (%f) <- %f}\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices)
            Sexp.pp_hum
            ([%sexp_of: int array] idcs)
            v1 result;
        set_from_float ptr idcs result
    | Set (ptr, indices, llv) -> (
        if !debug_verbose_trace then
          Caml.Format.printf "{TRACE: %a [%a] <- ...\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices);
        let idcs = lookup env indices in
        let result = loop_float env llv in
        if !debug_verbose_trace then
          Caml.Format.printf "TRACE: %a [%a -> %a] (%f) <- %f}\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices)
            Sexp.pp_hum
            ([%sexp_of: int array] idcs)
            (get_as_float ptr idcs) result;
        try set_from_float ptr idcs result
        with e ->
          Caml.Format.printf "ERROR: %a [%a -> %a] <- %f -- indices out of bounds\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices)
            Sexp.pp_hum
            ([%sexp_of: int array] idcs)
            result;
          Node.print_node_preamble @@ get ptr.id;
          raise e)
    | Set_local (id, llv) -> locals := Map.update !locals id ~f:(fun _ -> loop_float env llv)
    | Comment message when !with_debug && !executor_print_comments -> Stdio.printf "%s\n%!" message
    | Staged_compilation exp -> exp ()
    | Synchronize _ ->
        (* Assuming the interpreter is single-threaded. *)
        ()
    | Dynamic_indices { tensor; tensor_idcs; dynamic_idcs; target_dims; body; slice = _ } ->
        dynamic_indices env
          (Option.value_exn @@ get_tensor tensor)
          ~tensor_idcs ~dynamic_idcs ~target_dims body
    | Comment c ->
        if !debug_verbose_trace then (
          Caml.Format.printf "TRACE: %s -- prior state of nodes: {\n%!" c;
          Ndarray.print_decimals_precision := 9;
          for i = 1 to session_state.next_session_id - 1 do
            Caml.Format.printf "TRACE: %a\n%!" PrintBox_text.pp
              (Node.to_printbox ~single_node:true ~with_grad:true ~depth:9 @@ get i)
          done;
          Caml.Format.printf "}\n%!")
  and loop_float env llv =
    let open Float in
    let loop = loop_float env in
    match llv with
    | Constant c -> c
    | Get (ptr, indices) ->
        if !debug_verbose_trace then
          Caml.Format.printf "{TRACE: %a [%a] -> ...\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices);
        let idcs = lookup env indices in
        let result =
          try get_as_float ptr idcs
          with e ->
            Caml.Format.printf "ERROR: %a [%a -> %a] -- indices out of bounds\n%!" Sexp.pp_hum
              ([%sexp_of: Ndarray.ptr] ptr)
              Sexp.pp_hum
              ([%sexp_of: Indexing.axis_index array] indices)
              Sexp.pp_hum
              ([%sexp_of: int array] idcs);
            Node.print_node_preamble @@ get ptr.id;
            raise e
        in
        if !debug_verbose_trace then
          Caml.Format.printf "TRACE: %a [%a -> %a] -> %f}\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] ptr)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] indices)
            Sexp.pp_hum
            ([%sexp_of: int array] idcs)
            result;
        result
    | Local_scope { id; prec = _; body; orig_indices } ->
        if !debug_verbose_trace then
          Caml.Format.printf "{TRACE: %a [%a] <-> ...\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] id.tensor)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] orig_indices);
        let old_locals = !locals in
        locals := Map.update !locals id ~f:(fun _ -> 0.0);
        loop_proc env body;
        let result = Map.find_exn !locals id in
        locals := old_locals;
        let idcs = lookup env orig_indices in
        if !debug_verbose_trace then
          Caml.Format.printf "TRACE: %a [%a / %a] (%f) <-> %f}\n%!" Sexp.pp_hum
            ([%sexp_of: Ndarray.ptr] id.tensor)
            Sexp.pp_hum
            ([%sexp_of: Indexing.axis_index array] orig_indices)
            Sexp.pp_hum
            ([%sexp_of: int array] idcs)
            (get_as_float id.tensor idcs) result;
        result
    | Get_local id -> Map.find_exn !locals id
    | Get_global (C_function _) -> failwith "NOT IMPLEMENTED YET: jit-dynloading C calls in the interpreter"
    | Binop (Arg1, llv1, _llv2) -> loop llv1
    | Binop (Arg2, _llv1, llv2) -> loop llv2
    | Binop (op, llv1, llv2) -> interpret_binop op (loop llv1) (loop llv2)
    | Unop (Identity, llv) -> loop llv
    | Unop (Relu, llv) ->
        let v = loop llv in
        if v > 0.0 then v else 0.0
  and dynamic_indices env tensor ~tensor_idcs ~dynamic_idcs ~target_dims body =
    let env =
      Array.foldi dynamic_idcs ~init:env ~f:(fun provider_dim env key ->
          let idcs = lookup ~provider_dim env tensor_idcs in
          let actual =
            try Nd.get_as_int tensor idcs
            with e ->
              Caml.Format.printf "ERROR: dynamic index at [%a -> %a] -- indices out of bounds\n%!" Sexp.pp_hum
                ([%sexp_of: Indexing.axis_index array] tensor_idcs)
                Sexp.pp_hum
                ([%sexp_of: int array] idcs);
              raise e
          in
          let target_dim = target_dims.(provider_dim).dim in
          Map.add_exn ~key ~data:(actual % target_dim) env)
    in
    loop_proc env body
  in
  loop_proc empty_env llc



(** Constructs a node with empty tensors of the specified precision and registers it in the global store.
    Note that the precision for gradients should not be lower than the precision for values. *)
let create ~(value_prec : Nd.prec) ?(grad_prec : Nd.prec option) ?(literal = false) ()
    ~op_label ?desc_label ~axis_labels ~default_display_indices ~children annot =
  let node =
    match value_prec with
    | Void_prec -> assert false
    | Half_prec value_prec -> (
        match grad_prec with
        | None -> create_node ~value_prec ()
        | Some Void_prec -> assert false
        | Some (Half_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Single_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Double_prec grad_prec) -> create_node ~value_prec ~grad_prec ())
    | Single_prec value_prec -> (
        match grad_prec with
        | None -> create_node ~value_prec ()
        | Some Void_prec -> assert false
        | Some (Half_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Single_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Double_prec grad_prec) -> create_node ~value_prec ~grad_prec ())
    | Double_prec value_prec -> (
        match grad_prec with
        | None -> create_node ~value_prec ()
        | Some Void_prec -> assert false
        | Some (Half_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Single_prec grad_prec) -> create_node ~value_prec ~grad_prec ()
        | Some (Double_prec grad_prec) -> create_node ~value_prec ~grad_prec ())
  in
  { id = node.id; node; op_label; desc_label; children; axis_labels; default_display_indices; annot; literal }

let create_of_same_precision_as ?literal node =
  match (node.value, node.grad) with
  | Single_nd _, (Some (Single_nd _) | None) ->
      create ~value_prec:Nd.single ~grad_prec:Nd.single ?literal ()
  | Single_nd _, Some (Double_nd _) ->
      create ~value_prec:Nd.single ~grad_prec:Nd.double ?literal ()
  | Double_nd _, (Some (Double_nd _) | None) ->
      create ~value_prec:Nd.double ~grad_prec:Nd.double ?literal ()
  | _, Some grad ->
      invalid_arg @@ "create_of_same_precision_as: unsupported combination of precisions value: "
      ^ Nd.precision_string node.value ^ ", grad: " ^ Nd.precision_string grad
  | _ ->
      invalid_arg @@ "create_of_same_precision_as: unsupported combination of precisions value: "
      ^ Nd.precision_string node.value

let create_of_promoted_precision n1 n2 =
  match (n1.value, n2.value) with
  | Single_nd _, Single_nd _ -> (
      match (n1.grad, n2.grad) with
      | _, Some (Double_nd _) | Some (Double_nd _), _ ->
          create ~value_prec:Nd.single ~grad_prec:Nd.double ()
      | _ -> create ~value_prec:Nd.single ~grad_prec:Nd.single ())
  | _, Double_nd _ | Double_nd _, _ -> create ~value_prec:Nd.double ~grad_prec:Nd.double ()
  | _ ->
      invalid_arg @@ "create_of_promoted_precision: unsupported combination of precisions n1 value: "
      ^ Nd.precision_string n1.value ^ ", n2 value: " ^ Nd.precision_string n2.value



(* FIXME: not inlining here gives an error about PrintBox.Simple.t_of_sexp missing *)
type printbox =
  (* PrintBox.Simple.t *)
  [ `Empty
  | `Hlist of printbox list
  | `Pad of printbox
  | `Table of printbox array array
  | `Text of string
  | `Tree of printbox * printbox list
  | `Vlist of printbox list ]
[@@deriving sexp, compare]



let setup_array hs_pat (hs_typ, slot, hs) =
  let loc = hs.pexp_loc in
  match hs_typ with
  | Tensor ->
      ( Some (hs_pat, hs, [%expr [%e pat2expr hs_pat].nondiff_forward_body]),
        hs_typ,
        slot,
        [%expr CDSL.value_of_id [%e pat2expr hs_pat].id] )
  | Unknown | Tensor_or_node_or_data -> (None, hs_typ, slot, [%expr CDSL.value_of_id [%e hs].id])
  | Grad_of_source expr -> (None, hs_typ, slot, [%expr CDSL.grad_of_id [%e expr].id])
  | Grad_of_code expr ->
      (Some (hs_pat, hs, pat2expr hs_pat), hs_typ, slot, [%expr CDSL.grad_of_id [%e data_of_code expr].id])
  | Code -> (Some (hs_pat, hs, pat2expr hs_pat), hs_typ, slot, data_of_code hs)

let setup_node_id hs_pat (hs_typ, slot, hs) =
  let loc = hs.pexp_loc in
  match hs_typ with
  | Tensor ->
      ( Some (hs_pat, hs, [%expr [%e pat2expr hs_pat].nondiff_forward_body]),
        hs_typ,
        slot,
        [%expr [%e pat2expr hs_pat].id] )
  | Grad_of_source expr -> (None, hs_typ, slot, [%expr [%e expr].id])
  | Unknown | Tensor_or_node_or_data -> (None, hs_typ, slot, [%expr [%e hs].id])
  | Grad_of_code expr -> (Some (hs_pat, hs, pat2expr hs_pat), hs_typ, slot, [%expr [%e data_of_code expr].id])
  | Code -> (Some (hs_pat, hs, pat2expr hs_pat), hs_typ, slot, [%expr [%e data_of_code hs].id])

]}
