{[
  let big_range = Array.init 300 ~f:(Int.to_float) in
  let r_data = TDSL.data ~label:"big_range" ~batch_dims:[2] ~output_dims:[3;5]
      (fun ~n:_ -> Init_op (Constant_fill big_range)) in
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 0.00e+0  1.00e+0  2.00e+0  3.00e+0  4.00e+0 │ 1.50e+1  1.60e+1  1.70e+1  1.80e+1  1.90e+1 ││
    ││      │ 5.00e+0  6.00e+0  7.00e+0  8.00e+0  9.00e+0 │ 2.00e+1  2.10e+1  2.20e+1  2.30e+1  2.40e+1 ││
    ││      │ 1.00e+1  1.10e+1  1.20e+1  1.30e+1  1.40e+1 │ 2.50e+1  2.60e+1  2.70e+1  2.80e+1  2.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 3.00e+1  3.10e+1  3.20e+1  3.30e+1  3.40e+1 │ 4.50e+1  4.60e+1  4.70e+1  4.80e+1  4.90e+1 ││
    ││      │ 3.50e+1  3.60e+1  3.70e+1  3.80e+1  3.90e+1 │ 5.00e+1  5.10e+1  5.20e+1  5.30e+1  5.40e+1 ││
    ││      │ 4.00e+1  4.10e+1  4.20e+1  4.30e+1  4.40e+1 │ 5.50e+1  5.60e+1  5.70e+1  5.80e+1  5.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
  refresh_session ();
  print_tensor ~with_code:false ~with_grad:false `Default @@ r_data;
  [%expect {|
    ┌────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │[1]: <big_range> shape 0:2|1:3,2:5                                                                  │
    │┌──────┬─────────────────────────────────────────────┬─────────────────────────────────────────────┐│
    ││      │0 @ 0                                        │1 @ 0                                        ││
    ││      │axis 2                                       │axis 2                                       ││
    │├──────┼─────────────────────────────────────────────┼─────────────────────────────────────────────┤│
    ││axis 1│ 6.00e+1  6.10e+1  6.20e+1  6.30e+1  6.40e+1 │ 7.50e+1  7.60e+1  7.70e+1  7.80e+1  7.90e+1 ││
    ││      │ 6.50e+1  6.60e+1  6.70e+1  6.80e+1  6.90e+1 │ 8.00e+1  8.10e+1  8.20e+1  8.30e+1  8.40e+1 ││
    ││      │ 7.00e+1  7.10e+1  7.20e+1  7.30e+1  7.40e+1 │ 8.50e+1  8.60e+1  8.70e+1  8.80e+1  8.90e+1 ││
    │└──────┴─────────────────────────────────────────────┴─────────────────────────────────────────────┘│
    └────────────────────────────────────────────────────────────────────────────────────────────────────┘ |}];
]}

{[
let save_tensors ~name =
  let out = Npy.Npz.open_out (name ^ ".npz") in
  Hashtbl.iter Low_level.global_node_store ~f:(fun v ->
      let save field arr = Npy.Npz.write out Node.(tensor_ptr_name { id = v.id; field }) arr in
      let f arr = save Value arr in
      Nd.map { f } v.node.value;
      let f arr = save Grad arr in
      Option.iter v.node.grad ~f:(Nd.map { f }))

(** Restores the content of already-existing tensors from the file [name ^ ".npz"]. With [~partially:true],
    does not complain about tensors missing in the file. *)
let restore_tensors ?(partially = false) f_name =
  let inp = Npy.Npz.open_in (f_name ^ ".npz") in
  Hashtbl.iteri Low_level.global_node_store ~f:(fun ~key:id ~data:n ->
      let restore field =
        let ptr = Node.{ id; field } in
        match Low_level.get_tensor ptr with
        | None -> ()
        | Some arr ->
            let t_name = Node.(tensor_ptr_name { id = v.id; field }) in
            let src = Npy.Npz.read inp t_name in
            let f prec dst =
              match Npy.to_bigarray Bigarray.c_layout (Nd.precision_to_bigarray_kind prec) src with
              | None -> if not partially then failwith ("Session.restore_tensors: missing tensor " ^ t_name)
              | Some src -> Nd.A.blit src dst
            in
            Nd.map_with_prec { f } arr
      in
      restore Value;
      restore Node.Grad)
]}

Exec_as_cuda [~is_initial] and [~is_final]:
{[
    if is_initial then (
      if verbose then Stdio.printf "Exec_as_cuda.jit: copying host-to-device\n%!";
      List.iter arrays ~f:(function
        | ptr, { hosted = Some ndarray; global = Some name; global_ptr = Some (lazy dst); size_in_elems; _ }
          ->
            let tn = Hashtbl.find_exn traced_store ptr in
            if tn.read_before_write then (
              let f src = Cu.memcpy_H_to_D ~length:size_in_elems ~dst ~src () in
              if verbose && !Low_level.with_debug then
                Stdio.printf "Exec_as_cuda.jit: memcpy_H_to_D for %s, length: %d\n%!" name size_in_elems;
              Ndarray.map { f } ndarray)
        | _ -> ()));


if is_final then (
      if verbose then Stdio.printf "Exec_as_cuda.jit: copying device-to-host\n%!";
      List.iter arrays ~f:(function
        | ptr, { hosted = Some ndarray; global = Some name; global_ptr = Some (lazy src); size_in_elems; _ }
          ->
            let tn = Hashtbl.find_exn traced_store ptr in
            if not tn.read_only then (
              let f dst = Cu.memcpy_D_to_H ~length:size_in_elems ~dst ~src () in
              if verbose && !Low_level.with_debug then
                Stdio.printf "Exec_as_cuda.jit: memcpy_D_to_H for %s\n%!" name;
              Ndarray.map { f } ndarray)
        | _ -> ()));


  let finalizers =
    Array.of_list arrays
    |> Array.filter_map ~f:(fun (_, tn) ->
           match tn.mem with
           | Device_finally_host ->
               Option.map2 tn.local tn.global ~f:(fun l_name g_name ->
                   let b = Buffer.create 4096 in
                   let ppf = Caml.Format.formatter_of_buffer b in
                   let body idcs =
                     Low_level.Staged_compilation
                       (fun () ->
                         Caml.Format.fprintf ppf "@[<2>%s[%a] =@ %s[%a];@]" g_name pp_array_offset
                           (idcs, tn.dims) l_name pp_array_offset (idcs, tn.dims))
                   in
                   let loops = Low_level.loop_over_dims tn.dims ~body in
                   jit_code ~traced_store ppf loops;
                   Caml.Format.pp_print_newline ppf ();
                   Buffer.contents b)
           | _ -> None)
  in
{|
  /* Finalization: copy local-to-global. */
  if (is_final) {
    %{String.concat_array ~sep:"\n    "
    @@ Array.map finalizers ~f:(String.substr_replace_all ~pattern:"\n" ~with_:"\n    ")}
  } 
|}
        
]}

Exec_as_gccjit [~is_initial] and [~is_final]:
{[
Option.iter hosted_ptr ~f:(fun hosted_ptr ->
            if is_local_finally_host mem then
              Block.eval finalize_block
              @@ RValue.call ctx (Function.builtin ctx "memcpy")
                   [
                     cast_void hosted_ptr;
                     cast_void @@ LValue.address @@ Option.value_exn local;
                     RValue.int ctx c_index size_in_bytes;
                   ]);

]}

Exec_as_cuda constants:
{[
  let constant_defs =
    List.filter_map arrays ~f:(fun (ptr, tn) ->
        match tn.mem with
        | Constant ->
            Option.map tn.global ~f:(fun t_name ->
                "__constant__ " ^ tn.num_typ ^ " " ^ t_name ^ "[" ^ Int.to_string tn.size_in_elems
                ^ if (Hashtbl.find_exn traced_store ptr).zero_initialized then "] = {0};" else "];")
        | _ -> None)
  in

{|
%{String.concat ~sep:"\n" constant_defs}
|}

        match mem with
        | Constant ->
            lazy
              (let ptr, size =
                 (* Defer till after compilation, to access the compiled-into module. *)
                 Cudajit.module_get_global
                   (Option.value_exn session_state.last_module)
                   ~name:(LA.name v)
               in
               assert (Unsigned.Size_t.to_int size = size_in_bytes);
               ptr)
        | _ ->


]}

Ndarray:
{[
let get_as_int arr idx =
  let f x =
    let v = A.get x idx in
    try Float.to_int v
    with Invalid_argument _ ->
      Stdio.eprintf "\nRuntime error: Ndarray.get_as_int invalid float: %f\n%!" v;
      0
  in
  map { f } arr


let fold ~init ~f arr =
  let f arr = fold_bigarray ~init ~f arr in
  map { f } arr

]}